---
title: "DSC365: Introduction to Data Science"
author: "Web-Scraping"
date: "November 11, 2025"
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

## Resources

- "Web Scraping in R: rvest Tutorial": https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
- "rvest: easy web scraping with R": https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/
- "Web scraping tutorial in R": https://towardsdatascience.com/web-scraping-tutorial-in-r-5e71fd107f32

## Web scraping?

__Web scraping__ is the process of extracting information from websites automatically and transforming it into a structured data set.

- There's TONS of data freely available online, but...
- ...the data is provided in an unstructured format: you can always copy & paste, but it's time-consuming and prone to errors.

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse)
```

## What is webscraping? 

- Extract data from websites 
    + Tables
    + Links to other websites
    + Text
    
Two different scenarios:

1. Screen scraping 
2. Web APIs (application programming interface)

__Why R?__ It includes all tools necessary to do web scraping, familiarity, direct analysis of data... 

- But other languages like Python and Java are also efficient tools.
    

## Why webscrape? 

- Because copy-paste is awful 
- Because it's fast
- Because you can automate it

## Before scraping, do some googling!

- If resource is well-known (e.g. Twitter, Fitbit, etc.), there is *probably* an existing R package for it.
- [ropensci](https://ropensci.org/) has a [ton of R packages](https://ropensci.org/packages/) providing easy-to-use interfaces to open data.
- The [Web Technologies and Services CRAN Task View](http://cran.r-project.org/web/views/WebTechnologies.html) is a great overview of various tools for working with data that lives on the web in R.


## A web of *messy* data!

- Recall the concept of [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf).
- Data is in a table where
    * 1 row == 1 observation
    * 1 column == 1 variable (observational attribute)
- Parsing web data (HTML/XML/JSON) is easy (for computers)
- Getting it in a tidy form is typically *not easy*.
- Knowing a bit about modern tools & web technologies makes it *much* easier.


## Hypertext Markup Language (HTML)

Most of the data on the web is still largely available as HTML - while it is structured (hierarchical/tree based) it often is not available in a form useful for analysis (flat / tidy).

```html
<html>
  <head>
    <title>This is a title</title>
  </head>
  <body>
    <p align="center">Hello world!</p>
  </body>
</html>
```
    

## `rvest`

`rvest` is a package that makes basic processing and manipulation of HTML data straight forward.

```{r}
#install.packages('rvest')
library(rvest)
```

`rvest` core functions:

- `read_html` - read HTML data from a url or character string.
- `html_nodes` - select specified nodes from the HTML document using CSS selectors.
  + extracts all elements from the page `x` that have the tag / class / id `path`. (Use SelectorGadget to determine `path`.)
- `html_table` - parse an HTML table into a data frame.
- `html_text` - extract tag pairs' content.
  + extracts all text from the nodeset `x` 
- `html_name` - extract tags' names.
- `html_attrs` - extract all of each tag's attributes.
- `html_attr` - extract tags' attribute value by name.


## SelectorGadget

+ SelectorGadget is a javascript bookmark let to determine the css selectors of pieces of a website we want to extract.
+ Bookmark the SelectorGadget link, https://rvest.tidyverse.org/articles/selectorgadget.html, then click on it to use it (or add the chrome extension)
+ When SelectorGadget is active, pieces of the website are highlighted in orange/green/red.

### Star Wars Example

Let's start with a simple example

1. Open: https://rvest.tidyverse.org/articles/starwars.html

2. Tell R what web-site we want to scrape

```{r}
html <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
```

3. Our goal is to turn this data into a 7 row data frame with variables title, year, director, and plot summary

**Method 1**: Get Elements Individually
```{r}
title <- html %>% html_nodes("#main h2") %>% html_text2()
year <- html %>% html_nodes("section > p:nth-child(2)") %>% html_text2() 
director <- html %>% html_nodes(".director") %>% html_text2() 
intro <- html %>% html_nodes(".crawl") %>% html_text2() 


star_wars = tibble(
  title = title,
  release = year,
  director = director,
  summary = intro
)
```


**Method 2**: Pull Entire Section at once.

```{r}
section <- html %>% html_elements("section")

star_wars2<- tibble(
  title = section %>% 
    html_element("h2") %>% 
    html_text2(),
  released = section %>% 
    html_element("p") %>% 
    html_text2() %>% 
    str_remove("Released: ") %>%
    parse_date(),
  director = section %>% 
    html_element(".director") %>% 
    html_text2(),
  intro = section %>% 
    html_element(".crawl") %>% 
    html_text2()
)

```


## Your Turn

1. Use SelectorGadget on https://www.baseball-reference.com/teams/KCR/2025-roster.shtml.
2. Pull from "Current 40 Man Roster" Table Only
  + Player's Name
  + Player's Age
  + Player's Position
  + Player's first year in MLB (`1stYr`)

```{r}
url <- "https://www.baseball-reference.com/teams/KCR/2025-roster.shtml"
html <- read_html(url)

names <- html %>% html_nodes("#the40man a") %>% html_text()
position <- html %>% html_nodes("#the40man .left+ .right") %>% html_text()
age <- html %>% html_nodes("#the40man .right:nth-child(8)") %>% html_text()
first_year <- html %>% html_nodes("#the40man .right:nth-child(14)") %>% html_text()

royals_2025 = tibble(
  Player = names,
  Position = position,
  Age = age,
  First_MLB_Year = first_year
)

```


## Some issues with Web-Scarping

1. Data not always able to be scraped

```{r}
url <- "https://www.nhl.com/hurricanes/roster"
html <- read_html(url)
html %>% html_nodes(".dQnpcR") %>% html_text()
```

- Data not living on this page, so nothing to scrape, it's pulling it in from somewhere else to populate the page


2. Check to make sure you're allowed! Weather.com allows users to access the data...

```{r warning=FALSE}
# install.packages("robotstxt")
library(robotstxt)
paths_allowed("https://weather.com")
```

... while other sites like Twitter (now) don't.

```{r warning=FALSE}
paths_allowed("https://x.com")
```

Another way to do that is just append “/robots.txt” to the end of the URL of the website you are targeting, which will also tell you whether the website is allowed to be scraped.  



## Another Example: Weather Data

Let's scrape the 10 Day Forecast!

```{r}
url <- "https://weather.com/weather/tenday/l/634980566a55b1d28086dff3f66d55615f823b4ff07f3056f8fafbad378fb85e"
html <- read_html(url)

dates <- html %>% html_nodes(".DetailsSummary--daypartName--CcVUz") %>% html_text()

high_temp <- html %>% html_nodes(".DetailsSummary--highTempValue--VHKaO") %>% html_text()

low_temp <- html %>% html_nodes(".DetailsSummary--lowTempValue--ogrzb") %>% html_text()

forecast_slo = tibble(
  date = dates,
  high = high_temp,
 low = low_temp
)
```

## Working with Tables

If you’re lucky, your data will be already stored in an HTML table, and it’ll be a matter of just reading it from that table

```{r}
url <- "https://www.baseball-reference.com/teams/KCR/2025-roster.shtml"
html <- read_html(url)

table <- html |> 
  html_element("table") |> 
  html_table()

```

## Your Turn 

**Option 1:**

1. Find a website that you may want to scrape (i.e. FDA, Census, NCAA Stats, Creighton Athletics, etc.) 
2. Read the URL into R and save the url as an object. 
3. Using the SelectorGadget on your website and look for a node with data that you want to collect.

**Option 2:**

See link in BlueLine ((Start of a) Webscraping Demo) for Dr. Kunin's `course_catalog.qmd` example (in BlueLine) where he was attempting to scrape information from Creighton's Course Catalog. It has some "fun" uses of Regular Expressions. Read through his thought process and see if you can understand what's going on.

1. Verify that the code still works
2. Add the missing pieces to the table using Regular Expressions (Number of Credits and FA/SP class) - I would use your AI of choice to help with the regular expressions
3. Use this thought process to try and scrape the webpage for your major to get the information in a nice dataframe (Use SelectorGadget)



