---
title: 'Week 11: Text mining'
author: 'MTH 365: Introduction to Data Science'
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(textdata)
```

## Recommended Reading

- _Text Mining with R_ (https://www.tidytextmining.com/)

Text data is one of the most common type of data in the modern world. Usually, it is much more complicated that the numerical data thus we need to pre-process the data before we use it. 

## Game of Thrones Twitter data 

Following a data set that captures the release of all six Game of Thrones episodes from the eighth season that premiered on 14th of April, 2019 (https://www.kaggle.com/datasets/monogenea/game-of-thrones-twitter). We will briefly cover web scraping at the end of this course. 

```{r}
got <- read.csv("got_tweets.csv")
glimpse(got)
```

Even though it is always exciting to work with the social media data, its messiness can make the analysis hard. It usually has a lot of missing data and text data is free of format. 


Let's first focus on the tweet text itself and think about if we can create some variables that are easier to analysis from the text. 

However, before we extract the information, we need to convert all the text to lower cases (or upper if you preferred)

```{r}
got$lowertext = tolower(got$text)
glimpse(got)
```

### Convert the text to Yes/No question

Suppose, we want to know whether certain words in the text may increase the retweet. Recall the str_detect function in the previous labs. (The `stringr` package is a common package used for text data). 

```{r}
got = got %>% 
  mutate(isWatch = str_detect(lowertext, "watch"))
table(got$isWatch)

got = got %>% 
  mutate(isWinter = str_detect(lowertext, "winter"))
table(got$isWinter)
```

We can also look for a specific punctuation. 

```{r}
got = got %>% 
  mutate(isHash = str_detect(lowertext, "#"))
table(got$isHash)
```

We can even count how many hashtag in each text. 

```{r}
got = got %>% 
  mutate(NumHash = str_count(lowertext, "#"))
table(got$NumHash)
```

There are some other basic information you can extract, like the length of the text

```{r}
got = got %>% 
  mutate(length = str_length(lowertext))
summary(got$length)
```

### Parts of Speech

You can also do something fancy (but will take much longer time to run). For example, we can generate the part-of-speech tags for the words in the text and count how many noun/verb in the text. It will be better to use the text without lower case transformation in the following detection. 

```{r}
library(udpipe)

detect_ps = function(text){
  tokens = udpipe::udpipe(text, "english")
  NumNoun = tokens %>% filter(upos == "PROPN") %>% summarise(N = n()) %>% pull()
  NumVerb = tokens %>% filter(upos == "VERB") %>% summarise(N = n()) %>% pull()
  return(c(NumNoun, NumVerb))
}

detect_ps("	
I canâ€™t stop humming the Game of Thrones theme music")

result = sapply(got$text[1:100], detect_ps)

got_sub = got[1:100,] %>% 
  mutate(numNoun = result[1,], 
         numVerb = result[2,])

glimpse(got_sub)
```

Other techinques like NER (Named entity recognition) can identify whether there is a Human name/Place in the text. See the tutorial if you are interested: https://rstudio-pubs-static.s3.amazonaws.com/230718_6d29404ebb2a444fa0d218917dd1ec67.html

### Sentimental analysis

Words has meaning, thus an emotion can be associated with each word, and the text. Sentimental score is usually a number. If it is positive, it indicates the word/sentence has a positive meaning and vice versa. The scale for sentiment scores using the `syuzhet` method is a decimal and ranges from -1 (most negative) to +1 (most positive). If have a line of text, the function returns the sum of all the sentiment scores of meaningful words.

```{r}
library(syuzhet)

get_sentiment("happy", method = "syuzhet")
get_sentiment("sad", method = "syuzhet")
get_sentiment("It was a nice day even though I lost the game. ", method = "syuzhet")

got$syuzhet = get_sentiment(got$text, 
                              method = "syuzhet")
summary(got$syuzhet)
```

### Try by yourself

Following is a sample IMDB people data. 

```{r}
imdb= read.csv("imdb.csv")
glimpse(imdb)
```

Create some other variables based on this data: 
1. Whether the person is an actor
2. How many profession that each person has
3. Whether the person is still alive

```{r}
imdb = imdb %>% 
  mutate(isActor = str_detect(primaryProfession, "actor"))
table(imdb$isActor)

imdb = imdb %>% 
  mutate(Numpro = str_count(primaryProfession, ",")+1)
table(imdb$Numpro)

imdb = imdb %>% 
  mutate(isAlive = !is.na(deathYear))
table(imdb$isAlive)
```

### Clean the categorical levels

Sometimes we need to clean the data to generate a data that easier for analysis. Suppose now we only take everyone's first profession. We can find patterns in the data using regular expressions (https://www.datacamp.com/tutorial/regex-r-regular-expressions-guide)

```{r}
imdb$firstPro = gsub(",.*", "", imdb$primaryProfession)

head(imdb)
table(imdb$firstPro)

imdb = imdb %>% mutate(cleanPro = case_when(
  firstPro == "actor" | firstPro == "actress" ~ "act",
  str_detect(firstPro, "director") ~ "director",
  firstPro == "writer" | firstPro == "editor" ~ "write",
  firstPro == "producer" ~ "producer",
  TRUE ~ "other"
))
table(imdb$cleanPro)
```

One more note: the date/time data can also be treated as the text data. You can choose to extract only the year/month from the complete timestamps. 

### Summarize the text

Go back to our Game of Thrones example. Instead of generating new variables to fit the model, sometimes we just what to know what those text are about. Smart non-coding way includes generate a word cloud, which displays the most frequent words. 

It is difficult to analyze a sentence of text. We can't filter out words or count which occur most frequently since it is made up of multiple words. Hence, we need to covert it into *tokens*. 
  - A *token* is a meaningful unit of text that can be used for further analysis.

We create tokens using the `unnest_tokens` function.


```{r}
tweet_words <- got %>% 
  select(text) %>% 
  unnest_tokens(word,text)

glimpse(tweet_words)
```

```{r}
top25 <- tweet_words %>% count(word,sort=T) %>% slice(1:10) 

ggplot(top25, 
       aes(x = reorder(word, n, function(n) -n), y = n)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Stop words are words that does not have meaning in the text, like "the" and "a". 

```{r, warning=FALSE, message=FALSE}
glimpse(stop_words)

my_stop_words <- stop_words %>% select(word) %>% 
  bind_rows(data.frame(word = c("https", "t.co", "rt")))

tweet_words_interesting <- tweet_words %>% anti_join(my_stop_words)

tweet_words_interesting %>% 
  group_by(word) %>% 
  tally(sort=TRUE) %>% 
  slice(1:10) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


You can also create a Word Cloud (only in html) - https://r-graph-gallery.com/196-the-wordcloud2-library.html

```{r}
library(wordcloud2)

t <- tweet_words_interesting %>% 
  group_by(word) %>%
  tally(sort=TRUE) %>%
  filter(n > 500)

wordcloud2(data=t, size=1.6) 
```


You can also show the general trend of sentiments in all the words. 

```{r, warning=FALSE}
#library(textdata)

nrc_lex <- get_sentiments("nrc")
fn_sentiment <- tweet_words_interesting %>% left_join(nrc_lex)
fn_sentiment %>% filter(!is.na(sentiment)) %>% 
  group_by(sentiment) %>% 
  summarise(n=n())

bing_lex <- get_sentiments("bing")
fn_sentiment <- tweet_words_interesting %>% left_join(bing_lex)
fn_sentiment %>% filter(!is.na(sentiment)) %>% group_by(sentiment) %>% summarise(n=n())
```

