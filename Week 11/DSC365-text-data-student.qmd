---
title: "Text Mining"
author: 'DSC 365: Introduction to Data Science'
date: "November 5, 2024"
format: html
---

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(textdata)
```

## Recommended Reading

-   *Text Mining with R* (https://www.tidytextmining.com/)
-   *stringr cheat sheet* (https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)

Text data is one of the most common type of data in the modern world. Usually, it is much more complicated than the numerical data thus we need to pre-process the data before we use it.

-   Humans generally suck at spelling and typing, so text data is messy

## Taylor Swift Tweets data

Following a data set that captures tweets from Taylor Swift between 2009 and 2022. (https://www.kaggle.com/datasets/thedevastator/taylor-swift-s-social-media-performance-tweet-en). We will briefly cover web scraping at the end of this course.

```{r}
library(readr)
TaylorSwift13 <- read_csv("../Week 11/TaylorSwift13.csv")
glimpse(TaylorSwift13)
```

Even though it is always exciting to work with the social media data, its messiness can make the analysis hard. It usually has a lot of missing data and text data is free of format.

Let's first focus on the tweet text itself and think about if we can create some variables that are easier to analysis from the text.

However, before we extract the information, we need to convert all the text to lower cases (or upper if you preferred)

```{r}

```

### Convert the text to Yes/No question

Suppose, we want to know whether certain words in the text may increase the retweet. Recall the str_detect function in the previous labs. (The `stringr` package is a common package used for text data).

```{r}

```

We can also look for a specific punctuation.

```{r}

```

We can even count how many hashtag in each text.

```{r}

```

There are some other basic information you can extract, like the length of the text

```{r}

```

### Parts of Speech

You can also do something fancy (but will take much longer time to run). For example, we can generate the part-of-speech tags for the words in the text and count how many noun/verb in the text. It will be better to use the text without lower case transformation in the following detection.

```{r}
library(udpipe)

detect_ps = function(text){
  tokens = udpipe::udpipe(text, "english")
  NumNoun = tokens %>% filter(upos == "PROPN") %>% summarise(N = n()) %>% pull()
  NumVerb = tokens %>% filter(upos == "VERB") %>% summarise(N = n()) %>% pull()
  return(c(NumNoun, NumVerb))
}

detect_ps("	
surprise tonight at midnight i’ll be releasing my 8th studio album, folklore; an entire brand new album of songs i’ve poured all of my whims, dreams, fears, and musings into")

result = sapply(TaylorSwift13$content[1:100], detect_ps)

TaylorSwift13_sub = TaylorSwift13[1:100,] %>% 
  mutate(numNoun = result[1,], 
         numVerb = result[2,])

glimpse(TaylorSwift13_sub)
```

Other techniques like NER (Named entity recognition) can identify whether there is a Human name/Place in the text. See the tutorial if you are interested: https://rstudio-pubs-static.s3.amazonaws.com/230718_6d29404ebb2a444fa0d218917dd1ec67.html

### Sentimental analysis

Words has meaning, thus an emotion can be associated with each word, and the text. Sentimental score is usually a number. If it is positive, it indicates the word/sentence has a positive meaning and vice versa. The scale for sentiment scores using the `syuzhet` method is a decimal and ranges from -1 (most negative) to +1 (most positive). If have a line of text, the function returns the sum of all the sentiment scores of meaningful words.

```{r}
library(syuzhet)

get_sentiment("happy", method = "syuzhet")
get_sentiment("sad", method = "syuzhet")
get_sentiment("It was a nice day even though I lost the game. ", method = "syuzhet")

```

### Try by yourself

Following is a sample IMDB people data.

```{r}
imdb= read.csv("../Week 11/imdb.csv")
glimpse(imdb)
```

Create some other variables based on this data: 1. Whether the person is an actor 2. How many profession that each person has 3. Whether the person is still alive

```{r}

```

### Clean the categorical levels: Regular Expressions

*Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems. - Jamie Zawinski*

Sometimes we need to clean the data to generate a data that easier for analysis. We can find patterns in the data using regular expressions (https://www.datacamp.com/tutorial/regex-r-regular-expressions-guide)

-   A **regular expression** is a sequence of characters that specify a match pattern to search for in a larger text

```{r}
num_string <- "phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789"

ssn <- str_extract(num_string, "[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]")
ssn

phone <- str_extract(num_string, "[0-9]{3}.[0-9]{3}.[0-9]{4}")
phone
```

Suppose now we only take everyone's first profession.

```{r}
imdb$firstPro = gsub(",.*", "", imdb$primaryProfession)

head(imdb)
table(imdb$firstPro)

imdb = imdb %>% mutate(cleanPro = case_when(
  firstPro == "actor" | firstPro == "actress" ~ "act",
  str_detect(firstPro, "director") ~ "director",
  firstPro == "writer" | firstPro == "editor" ~ "write",
  firstPro == "producer" ~ "producer",
  TRUE ~ "other"
))

table(imdb$cleanPro)
```

### Separating/Extracting Information from Columns

Go back to our Taylor Swift example.

One more note: the date/time data can also be treated as the text data. You can choose to extract only the year/month from the complete timestamps.

```{r}

```

```{r}

```

### Find and Replace

```{r}
fruits <- c("one apple", "two pears", "three bananas")

str_replace_all(fruits, "[aeiou]", "-")
str_replace_all(fruits, c("one" = "1", "two" = "2", "three" = "3"))
```

### Summarize the text

Instead of generating new variables to fit the model, sometimes we just what to know what those texts are about. Smart non-coding way includes generate a word cloud, which displays the most frequent words.

It is difficult to analyze a sentence of text. We can't filter out words or count which occur most frequently since it is made up of multiple words. Hence, we need to covert it into *tokens*. - A *token* is a meaningful unit of text that can be used for further analysis.

We create tokens using the `unnest_tokens` function.

```{r}
tweet_words <- TaylorSwift13 %>% 
  dplyr::select(content) %>% 
  unnest_tokens(word,content)

glimpse(tweet_words)
head(tweet_words)
```

```{r}
top10 <- tweet_words %>% count(word,sort=T) %>% slice(1:10) 

ggplot(top10, 
       aes(x = reorder(word, n, function(n) -n), y = n)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Stop words are words that does not have meaning in the text, like "the" and "a".

```{r, warning=FALSE, message=FALSE}
glimpse(stop_words)

my_stop_words <- stop_words %>% dplyr::select(word) %>% 
  bind_rows(data.frame(word = c("https", "t.co", "rt", "amp")))

tweet_words_interesting <- tweet_words %>% anti_join(my_stop_words)

tweet_words_interesting %>% 
  group_by(word) %>% 
  tally(sort=TRUE) %>% 
  slice(1:10) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

You can also create a Word Cloud (only in html) - https://r-graph-gallery.com/196-the-wordcloud2-library.html

```{r}
library(wordcloud2)

```

You can also show the general trend of sentiments in all the words.

```{r, warning=FALSE}
#library(textdata)

nrc_lex <- get_sentiments("nrc")
fn_sentiment <- tweet_words_interesting %>% left_join(nrc_lex)
fn_sentiment %>% filter(!is.na(sentiment)) %>% 
  group_by(sentiment) %>% 
  summarise(n=n())

bing_lex <- get_sentiments("bing")
fn_sentiment <- tweet_words_interesting %>% left_join(bing_lex)
fn_sentiment %>% filter(!is.na(sentiment)) %>% group_by(sentiment) %>% summarise(n=n())
```
