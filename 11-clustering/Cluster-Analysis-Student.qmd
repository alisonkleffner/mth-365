---
title: "Cluster Analysis"
author: 'DSC 365: Introduction to Data Science'
date: "October 30, 2025"
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.align = "center", fig.width = 12, fig.height = 3)
```

# Recommended Reading

- *Modern Data Science with R* Ch 12: Unsupervised Learning

```{r, message=FALSE}
library(tidyverse)
```

# Unsupervised Learning

- **Unsupervised learning**: There is no response variable. We try to learn the patterns of the input data, usually by *clustering* them into several groups.


- Focus on two methods: Clustering vs. Principal Components
  + Clustering and principal components are two techniques that are designed to group the data together.
  + **Cluster Analysis**: group observation together based on similarities between observations
  + **Principal Component Analysis (PCA)**: group variables together and creates new variables that better represent the data
  
 Drawback to unsupervised methods - more subjective, as there is no universal way to validate results. 
  
# Cluster Analysis

## In General

- Partition observations into distinct groups so that observations in a group are similar and observations in different groups are quite different
  + Will need to define similar/different (can be domain specific)
  
## Data

We're going to work with the `gapminder` data set.

```{r}
library(dslabs)
library(tidyverse)
data(gapminder)
glimpse(gapminder)
```

Since each country is repeated by year, we're going to choose a single year to focus on.
  
  - Using `2015` since it is the most recent year with the most variables included
  
```{r}
gapminder_2015 <- gapminder %>% filter(year == 2015)
```

  

## Main Clustering Techniques

Great number of clustering methods! Going to focus on the two most common:

1. Hierarchical Clustering: once two observations are clustered (or separated) they will always be together in the same cluster (or separated)
  
  - Agglomerative **(focus)** - bottom-up
  - Divisive 

2. Non-Hierarchical: Data partitioned into an initial set of clusters, where cluster memberships is then altered. Can change clusters. 
  
  - K-Means **(focus)**
  - K-Medoid
  
  
## K-Means Clustering

Nonhierarchical clustering partitions the data into a pre-specified fixed number of non-overlapping clusters and rearranges the cluster memberships according to some optimization criteria. Here we are going to focus on K-means.

### Properties:

- Each observation must belong to a cluster
- No observation can be in more than one cluster


### Algorithm:

**Idea**: good clusters have within cluster variation that is as small as possible

  + Essentially want to solve the problem: $\text{minimize}_{C_1, \dots, C_K} (\sum^K_{k=1}W(C_k))$
    + Want total within cluster variation $(W(C_k))$ to be as small as possible
    + Many different ways to define $W(C_k)$. Using squared Euclidean distance is the most common (sum of all paired squared euclidean distances between observations in the $k^{th}$ cluster divided by the number of observations in the $k^{th}$ cluster).

**Problem:** very difficult minimization problem as there are so many ways to partition your n observations into k clusters.
  + **Solution:** find a local optimum
  
**Steps:**

1. Randomly assign a number (1,...,*k*) to each of the observations. These serve as the initial cluster assignments for the observations.
2. Iterate until the clusters stop changing. 
  + For each of the *k* clusters, compute cluster centroid (a vector of the *p* feature means for the observations of the $k^{th}$ cluster). 
  + Assign each observation to the cluster whose centroid is closest (generally using euclidean distance).
  
With these steps will reach a local optimum:
  + Results dependent on initial random cluster assignments for the observations in Step 1. 
    - Run k-means multiple times with different initial cluster memberships


### How do you choose *k*?

- No definitive answer. Somewhat subjective.
- [Some methods](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/#:~:text=A%20high%20average%20silhouette%20width,(Kaufman%20and%20Rousseeuw%201990))


### Code:

The data is already "categorized" by continent and region. Clustering allows us to find other, possibly unconsidered, similarities between points.

Let's start with a two-variable clustering (`fertility` and `life_expectancy`) on countries in the Americas.

```{r}
gap_fle <- gapminder_2015 %>%
  filter(continent == "Americas") %>%
  filter(fertility != 'NA') %>%
  select(fertility, life_expectancy)

k <- 4 #must first define number of clusters

set.seed(4) #make results repeatable
kmeans_clusters <- kmeans(gap_fle, centers = k, nstart = 10)
```

What does this look like?

```{r}
clust <- as.factor(kmeans_clusters$cluster)

gap_fle %>%
  ggplot(aes(x = fertility, y = life_expectancy, color = clust)) +
  geom_point()
```

What if we choose fewer clusters?

```{r}
k <- 3 #must first define number of clusters

set.seed(4) #make results repeatable
kmeans_clusters <- kmeans(gap_fle, centers = k, nstart = 10)

clust <- as.factor(kmeans_clusters$cluster)

gap_fle %>%
  ggplot(aes(x = fertility, y = life_expectancy, color = clust)) +
  geom_point()
```

#### Try Yourself

Now let's add a third variable: `infant_mortality`. Try with different $k$ values and make a data visualization with `fertility` and `life_expectancy`.

```{r}

```


## Hiearchical (Agglomerative)

Doesn't require that we know the number of clusters. Additionally, we are able to obtain a tree-based representation of observations called a **dendrogram**

- **Agglomerative** (bottom-up) clustering: start with observations in clusters by itself (leaves) and combine clusters up until everything in combined (trunk)

### Algorithm:

**Steps:**

1. Define a dissimilarity measure between each pair of observations (most often using Euclidean Distance). Start with each of the *n* observations in it's own cluster. 

2. Iterative:
  + Each of the *n* observations is in it's own cluster. The two clusters that are most similar (ie. smallest Euclidean distance) are fused, so now we have $n-1$ clusters.
  + Next two observations/clusters that are most similar are fused for $n-2$ clusters.

3. Proceed until all observations are fused together to belong to a single cluster.


**Problem:** how to define dissimilarity when have clusters of multiple observations
  - Linkage Method: how define dissimilarity between clusters
  - Two most common:
    + Average: compute all pairwise dissimilarities between Cluster A and Cluster B. Record the average of all the dissimilarities
    + Complete (default): compute all pairwise dissimilarities between Cluster A and Cluster B. Record the largest of the dissimilarities.

Output will depend on linkage method and dissimilarity measure (chosen based on data and research question at hand).

### Code

1. Calculate a measure of similarity between all pairs of observations.

- So let's calculate a "distance" measure for every pair of countries in North and South America. We'll start simple with a two-variable clustering (`fertility` and `life_expectancy`) on countries in the Americas.

```{r}
gap_fle <- gapminder_2015 %>%
  filter(continent == "Americas") %>%
  select(fertility, life_expectancy, country)

rownames(gap_fle) <- gap_fle$country

gap_dist <- dist(gap_fle[,1:2])

```

- Let's look at what is contained in `gap_dist`

```{r}
as.matrix(gap_dist)[1:5, 1:5] #don't really need to look at
```

2. Use a Clustering method to cluster observations based on the similarity values, where resulting clusterings hopefully have small within cluster variability and large among cluster variation. The resulting plot of the clustering is called a dendrogram.

```{r}
clusters <- hclust(gap_dist, method = "complete")

plot(clusters, hang = -1, cex = 0.5)
```


### Reading a Dendrogram:

- As we move up the tree, leaves begin to fuse into branches
  + The earlier in the tree these fusions happen, the more similar observations are
- Height (vertical axis) gives an idea of how different observations/clusters are 
  + Larger height -> more different
- Can't draw similarity conclusions based on horizontal distance, rather draw these conclusions based on the vertical distance  


### Obtaining a Certain number of clusters

If you just want cut the dendrogram to obtain only a certain number of clusters, can use the `cutree` function. Generally based on eye test for a sensible number.

```{r}
clust2 <- cutree(clusters, 4)
```

Can then visualize the clusters with the data

```{r, warning = FALSE}
gap_fle2 <- cbind(gap_fle, clust2)

gap_fle2 %>% 
  ggplot(aes(x = fertility, y = life_expectancy, color = as.factor(clust2))) +
  geom_point()
```


**Note:** clusters obtained by cutting a dendrogram are nested within clusters obtained by cutting the dendrogram at a greater height
  + This assumption might be unrealistic.


#### Try Yourself

Let's expand this, and add more variables (`infant_mortality`, `life_expectancy`, `fertility` and `popualtion`). Try to cluster using hierarchical clustering and visualize the dendrogram. For the sake of visualizing the dendrogram, just use the data for the Americas.

```{r, warning = FALSE}

```

# Practical Issues

## General:

- Should try several choices and chose one with more interpretable solution (k? linkage? dissimilarity measures?)
- No consensus method for validating clusters - work in progress
  + Clustering sub-groups or noise?
- Not guaranteed to be robust - different subsets create different clusters
- both of methods force observations into a cluster, when they might not actually belong to a cluster (outliers)
  + Mixture Models


## Standardizing Inputs

Sometimes, the results of the clustering are more dependent on the variation in variables in the data set, as Euclidean distance calculations uses non-standardized variables (assumes variables are identical).

- Generally, the variables with a bigger unit scale will contribute more to the Euclidean distance, leading to a biased estimate of clusters

```{r}
data_scale = gapminder_2015 %>%
  filter(continent == "Americas") %>%
  filter(infant_mortality != 'NA') %>%
  select(infant_mortality, life_expectancy) %>%
  scale() %>% as.data.frame()
```


```{r}
k <- 5

set.seed(4)
kmeans_clusters <- kmeans(data_scale, centers=k, nstart=10)


kmeans_clusters$cluster <- as.factor(kmeans_clusters$cluster)

ggplot(data_scale, aes(x=infant_mortality, y=life_expectancy)) + geom_point() + 
  aes(color=kmeans_clusters$cluster, pch=kmeans_clusters$cluster) +
  labs(y = "Life Expectancy", x = "Infant Mortality",
       color = "Cluster", shape = "Cluster")
```

#### Try Yourself

Now let's add a third variable: `infant_mortality`. Standardize the values and use k-means and hierarchical. Make a data visualization with `fertility` and `life_expectancy`. Compare this to data visualization for previous example


```{r}

```



# In Summary

**Hierarchical clustering**: 

- No need to decide the cluster number before the clustering
- Reproducible result 
- Computationally slow

**k-means clustering**: 

- Need to decide the cluster number first 
- Randomness may lead to different result
- Computationally fast

