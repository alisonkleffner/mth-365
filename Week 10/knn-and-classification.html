<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MTH365: Introduction to Data Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="KNN And Classification" />
    <meta name="date" content="2023-10-24" />
    <script src="libs/header-attrs-2.23/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# MTH365: Introduction to Data Science
]
.author[
### KNN And Classification
]
.date[
### October 24, 2023
]

---




&lt;style type="text/css"&gt;
.tiny .remark-code { /*Change made here*/
  font-size: 70% !important;
}
&lt;/style&gt;

## Announcements

- **Lab 6** due tonight at 11:59 pm

---

# Review on Random Forests

__Random forest__ : Every time, only select a part of data and a part of variables to fit the tree model. Repeat for multiple times, the classification result will be majority of all tree models and the regression result will be the average prediction of all tree models. 

Note: Random forest will expect you to have a relatively large number of input variables.  

--

**When to use random forests?**

1. When there are a lot of variables and you have no idea why one may be useful to explain the response variable.
2. Potential collinearity in the predictors.

Once the random forest tells you several potential important variables, you can try to fit linear model or decision tree for interpretation

---
## "Lazy" learning

So far we've focused on building models that can predict outcomes on a new set of data. Another approach is to just be _lazy_!

__Lazy learning__: no assumptions necessary to classify data

- How does that work?

__Example__: Consider the plot below - describe the relationship between x and y.

&lt;img src="knn-and-classification_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---
## "Lazy" learning

What if the data points belonged to three different groups, like this?

&lt;img src="knn-and-classification_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---
## "Lazy" learning

How should a new data point, `\((0.2, 0.5)\)` be classified? What about `\((0.4, 0.2)\)`?

&lt;img src="knn-and-classification_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;


---
## `\(k\)`-nearest neighbor (k-NN or KNN): 

1. In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). 
2. In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.

---
## k-NN Steps

In the k-NN algorithm, `\(k\)` specifies the number of neighbors and its algorithm is as follows:

- Choose the number `\(k\)` of neighbors
- Find the `\(k\)` Nearest Neighbor of an unknown data point using distance.
  + Euclidean Distance
- Among the `\(k\)`-neighbors, count the number of data points in each category.
- Assign the new data point to a category, where you counted the most neighbors.

---
## `knn()`

__Example__: Let's classify our new points using `\(k=2\)`.


```r
library(class)

knnModel = knn(train = data[,1:2], 
               test = new.points[,1:2], 
               cl = data$group, 
               k = 2, prob = TRUE)
knnModel
```

```
## [1] A C
## attr(,"prob")
## [1] 1 1
## Levels: A B C
```

---
## `knn()`

What if we use more points, `\(k=10\)`?


```r
knnModel = knn(train = data[,1:2], 
               test = new.points[,1:2], 
               cl = data$group, 
               k = 10, prob = TRUE)
knnModel
```

```
## [1] A C
## attr(,"prob")
## [1] 0.7 0.8
## Levels: A B C
```

---
## Advantages and disadvantages of `\(k\)`-nearest neighbors

The algorithm is easy to implement and straight forward. New data can be added seamlessly which will not impact the accuracy of the algorithm. No training period, relatively fast. 

However, sometimes it is hard to decide the k value. With more variables included, the accuracy will be affected. Sensitive to the outliers, you need to scale the data sometimes. 

---
## Example: Credit utilization

__Example__: Can we use kNN to predict which utilization quantile a new customer falls into based on their application data?


```r
library(ISLR)
data(Credit)
Credit &lt;- Credit %&gt;% mutate(Utilization = Balance/Limit) %&gt;% 
  mutate(Quartile = ifelse(Utilization&lt;0.01851, 'Q1', 
                           ifelse(Utilization&lt;0.09873, 'Q2',
                           ifelse(Utilization&lt;0.14325, 'Q3',
                                  'Q4'))))
```

---
## Example: Credit utilization

We want to predict the utilization quantile based on two application characteristics: credit rating and age.

&lt;img src="knn-and-classification_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---
## Example: Credit utilization

New applicants:

Name|Age|Credit Rating
---|---|---
Lacey|33|750
Zach|47|400
Ashlee|21|250


```r
applicants &lt;- data.frame(Age = c(33, 47, 21), 
                         Rating = c(750, 400, 250), 
                         Quartile=c('New', 'New', 'New'))
```


---
## Example: Credit utilization

Plotting the new applicants. Use `fct_relevel()` to reorder the categories.


```r
old = Credit %&gt;% dplyr::select(Age, Rating, Quartile)
full = rbind(old, applicants)
full = full %&gt;%
  mutate(Quartile = fct_relevel(Quartile,
                                "Q1", "Q2", "Q3", "Q4", "New"))
str(full$Quartile)
```

```
##  Factor w/ 5 levels "Q1","Q2","Q3",..: 2 3 2 3 2 3 2 3 2 4 ...
```

---
## Example: Credit utilization


```r
ggplot(full, aes(x=Age, y=Rating, group=Quartile)) + 
  geom_point(aes(col=Quartile, pch=Quartile)) + 
  scale_color_brewer(palette='Set1')
```

&lt;img src="knn-and-classification_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---
## Example: Credit utilization

Fit a first model with `\(k=10\)`.


```r
knn10 = knn(train = old[,1:2],
            test = applicants[,1:2],
            cl = old[,3], 
            k = 10, prob = TRUE)
knn10
```

```
## [1] Q3 Q4 Q2
## attr(,"prob")
## [1] 0.7 0.4 0.8
## Levels: Q1 Q2 Q3 Q4
```

---
## Example: Credit utilization

What if we use `\(k=20\)`?


```r
knn20 = knn(train = old[,1:2],
            test = applicants[,1:2],
            cl = old[,3], 
            k = 20, prob = TRUE)
knn20
```

```
## [1] Q3 Q4 Q2
## attr(,"prob")
## [1] 0.65 0.40 0.65
## Levels: Q1 Q2 Q3 Q4
```

---
## Example: Credit utilization
Going bigger: `\(k=100\)`


```r
knn100 = knn(train = old[,1:2],
            test = applicants[,1:2],
            cl = old[,3], 
            k = 100, prob = TRUE)
knn100
```

```
## [1] Q4 Q4 Q2
## attr(,"prob")
## [1] 0.44 0.46 0.45
## Levels: Q1 Q2 Q3 Q4
```

---
## Evaluate models with Cross-validation

__Example__: Let's add some more dimensions to the model. We want to know if k-nearest neighbor is effective at predicting quartile membership using an applicant's age, credit rating, income, number of existing credit cards, and education level. I'll randomly select 100 observations for testing, and assign the other 300 to my training data set.


```r
set.seed(365)
test_ID = sample(1:nrow(Credit), size = 100)
TEST = Credit[test_ID,]
TRAIN = Credit[-test_ID, ]
```

---
## Evaluate models with Cross-validation

Now, we'll set the testing data as "new data", and make predictions using the k-nearest neighbors from the training data.


```r
knn_train = TRAIN %&gt;% dplyr::select(Age, Rating, Income, Cards, Education)
knn_test = TEST %&gt;% dplyr::select(Age, Rating, Income, Cards, Education) 

knn50 = knn(train = knn_train, 
            test = knn_test,
            cl = TRAIN$Quartile, 
            k = 50, prob = TRUE)
knn50
```

```
##   [1] Q4 Q1 Q4 Q1 Q4 Q3 Q4 Q4 Q1 Q4 Q2 Q2 Q4 Q4 Q3 Q1 Q1 Q3 Q4 Q1 Q4 Q2 Q3 Q2 Q2 Q4 Q3 Q4 Q4 Q2 Q2 Q2 Q3 Q2 Q1 Q4 Q4
##  [38] Q3 Q3 Q2 Q1 Q2 Q3 Q4 Q1 Q4 Q3 Q2 Q4 Q4 Q4 Q2 Q4 Q1 Q4 Q1 Q1 Q2 Q1 Q1 Q1 Q4 Q1 Q4 Q4 Q1 Q4 Q4 Q4 Q4 Q2 Q1 Q4 Q2
##  [75] Q3 Q2 Q2 Q1 Q1 Q4 Q2 Q4 Q2 Q3 Q2 Q4 Q1 Q4 Q4 Q2 Q4 Q4 Q4 Q4 Q2 Q2 Q4 Q2 Q1 Q2
## attr(,"prob")
##   [1] 0.66 0.86 0.54 0.96 0.50 0.40 0.36 0.52 0.96 0.66 0.54 0.56 0.54 0.42 0.38 0.96 0.92 0.42 0.46 0.98 0.44 0.52
##  [23] 0.40 0.54 0.60 0.46 0.46 0.42 0.62 0.52 0.52 0.56 0.40 0.54 0.58 0.46 0.52 0.38 0.46 0.52 0.96 0.44 0.44 0.42
##  [45] 0.98 0.50 0.44 0.48 0.54 0.46 0.50 0.58 0.46 0.90 0.50 0.94 0.96 0.42 0.96 0.82 0.54 0.48 0.54 0.58 0.44 0.96
##  [67] 0.44 0.46 0.58 0.54 0.34 0.96 0.44 0.58 0.42 0.34 0.54 0.96 0.96 0.42 0.60 0.52 0.56 0.40 0.48 0.54 0.64 0.34
##  [89] 0.60 0.56 0.44 0.50 0.48 0.50 0.46 0.52 0.44 0.58 0.78 0.58
## Levels: Q1 Q2 Q3 Q4
```

---
## Evaluate models with Cross-validation

Now, we'll set the testing data as "new data", and make predictions using the k-nearest neighbors from the training data.


```r
#Create Confusion Matrix
t = table(knn50, TEST$Quartile)
t
```

```
##      
## knn50 Q1 Q2 Q3 Q4
##    Q1 18  2  1  1
##    Q2  4 16  5  1
##    Q3  0  3  7  2
##    Q4  0  8 16 16
```

```r
sum(diag(t))/nrow(TEST) #Classification Accuracy
```

```
## [1] 0.57
```


---
## Try by yourself

Let's see whether we can predict people's ethnicity based on their credit card information (Age, Rating, Income, Cards, Education, Balance). Try to fit a KNN model with K = 5, 10, 25, 50, 100. See how the prediction values change and think about why.

--


```
## [1] 0.9
## [1] 0.87
## [1] 0.82
## [1] 0.76
## [1] 0.72
```

---
## Summary 

Linear model: The basic model for regression, easy for interpretation, but has strict assumption thus hard to get a better prediction

Quadratic model: If you see a quadratic trend (in the EDA plot or residual plots) when fitting the linear model, you may consider quadratic model

Decision tree: Can be applied to regression and classification. Has good data visualization but it has high variance. 

Random Forest: a collection of tree models. Hard for interpretation but it can output variable importance. Can be useful if you have a lot of variable and what want to select the most useful ones. Also, if you have variables are not independent with each other, it performs better than the linear model. 

KNN：Can be applied to regression and classification. relatively fast, no assumption need and add data anytime. May affect the accuracy if we have too many variables and need to use CV to decide the k value. 
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
