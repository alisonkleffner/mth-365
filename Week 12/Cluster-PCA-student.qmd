---
title: "Cluster Analysis and Principal Component Analyssis"
author: 'DSC 365: Introduction to Data Science'
date: "November 12, 2024"
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Recommended Reading

- *Modern Data Science with R* Ch 12: Unsupervised Learning

```{r, message=FALSE}
library(tidyverse)
```

## Unsupervised Learning

- **Unsupervised learning**: There is no response variable. We try to learn the patterns of the input data, ususally by *clustering* them into several groups.

- Focus on two methods: Clustering vs. Principal Components
  + Clustering and principal components are two techniques that are designed to group the data together.
  + **Cluster Analysis** groups the observations that are similar to each other based on the distances between variables (how observations hang together)
  + **Principal Component Analysis (PCA)** groups the variables and creates new variables that better represents the data (how variables hang together).
  
## Cluster Analysis

### Data

We're going to work with the `gapminder` data set.

```{r}
library(dslabs)
data(gapminder)
glimpse(gapminder)
```

Since each country is repeated by year, we're going to choose a single year to focus on.
  
  - Using `2015` since it is the most recent year with the most variables included
  
```{r}
gapminder_2015 <- gapminder %>% filter(year == 2015)
```

  

### Main Clustering Techniques

1. Hierarchical Clustering: once two observations are clustered (or separated) they will always be together in the same cluster (or separated)
  
  - Agglomerative
  - Divisive 

2. Non-Hierarchical: Data partitioned into an initial set of clusters, where cluster memberships is then altered. Can change clusters. 


### Hiearchical

The data is already "categorized" by continent and region. Clustering allows us to find other, possibly unconsidered, similarities between points.

1. Calculate a measure of similarity between all pairs of observations. Often some sort of distance measure is used as the similarity measure.

- So let's calculate a "distance" measure for every pair of countries in North and South America. We'll start simple with a two-variable clustering (`fertility` and `life_expectancy`) on countries in the Americas.

```{r}
gap_fle <- gapminder_2015 %>%
  filter(continent == "Americas") %>%
  select(fertility, life_expectancy, country)
```

- Let's look at what is contained in `gap_dist`

```{r}

```

2. Use a Clustering method to cluster observations based on the similarity values, where resulting clusterings hopefully have small within cluster variability and large among cluster variation. The resulting plot of the clustering is called a dendrogram.

```{r}

```

If you just want cut the dendrogram to obtain only a certain number of clusters, can use the `cutree` function.

```{r}

```

Can then visualize the clusters with the data

```{r, warning = FALSE}

```


#### Try Yourself

Let's expand this, and add more variables (`infant_mortality`, `life_expectancy`, `fertility` and `popualtion`). Try to cluster using hierarchical clustering and visualize the dendrogram. For the sake of visualizing the dendrogram, just use the data for the Americas.

```{r, warning = FALSE}

```


### K-Means Clustering

Another option is to group observations without using a hierarchy. Nonhierarchical clustering partitions the data into a fixed number of non-overlapping groups and rearranges the cluster memberships according to some optimization criteria. Here we are going to focus on K-means.

Let's start once again with a two-variable clustering (`fertility` and `life_expectancy`) on countries in the Americas.

```{r}
k <- 4 #must define number of clusters

gap_fle <- gapminder_2015 %>%
  filter(continent == "Americas") %>%
  filter(fertility != 'NA') %>%
  select(fertility, life_expectancy)


```

What does this look like?

```{r}

```

What if we choose fewer clusters?

```{r}

```


How do you choose *k*?

- No definitive answer. Somewhat subjective.
- [Some methods](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/#:~:text=A%20high%20average%20silhouette%20width,(Kaufman%20and%20Rousseeuw%201990))

#### Try Yourself

Now let's add a third variable: `infant_mortality`. Try with different $k$ values and make a data visualization with `fertility` and `life_expectancy`.

```{r}

```


### Standardizing Inputs

Sometimes, the results of the clustering are more dependent on the variation in variables in the data set, as Euclidean distance calculations uses non-standardized variables (assumes variables are identical).

- Generally, the variables with a bigger unit scale will contribute more to the Euclidean distance, leading to a biased estimate of clusters

```{r}
data_scale = gapminder_2015 %>%
  filter(continent == "Americas") %>%
  filter(infant_mortality != 'NA') %>%
  select(infant_mortality, life_expectancy) %>%
  scale() %>% as.data.frame()
```


```{r}
k <- 5

set.seed(4)
kmeans_clusters <- kmeans(data_scale, centers=k, nstart=10)


kmeans_clusters$cluster <- as.factor(kmeans_clusters$cluster)

ggplot(data_scale, aes(x=infant_mortality, y=life_expectancy)) + geom_point() + 
  aes(color=kmeans_clusters$cluster, pch=kmeans_clusters$cluster)
```

#### Try Yourself

Now let's add a third variable: `infant_mortality`. Standardize the values and use k-means and hierarchical. Make a data visualization with `fertility` and `life_expectancy`. Compare this to data visualization for previous example


```{r}

```

```{r}
#Hierarchical

```


```{r}
#K-Means

```

### In Summary

**Hierarchical clustering**: 

- No need to decide the cluster number before the clustering
- Reproducible result 
- Computationally slow

**k-means clustering**: 

- Need to decide the cluster number first 
- Randomness may lead to different result
- Computationally fast

## Principal Component Analysis (PCA)

Sometimes a data set has a lot of variables. Some of these variables carry *lots* of information. Others carry *little to no* information. PCA can help reduce the dimensions of data by summarizing most of the variation into fewer dimensions called components. 

**PCA**: the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.

- Principal components: are a set of new orthogonal variables that are constructed as linear combinations
or mixtures of the initial variables that best fit the data.

### Data

The Happy Planet Index is a measure of a country’s ecological and societal “well-being”.

```{r}
library(readxl)
happyplanet2016 <- read_excel("happyplanet2016.xlsx")
glimpse(happyplanet2016)
```

Can we distill this down to a few important variables or ideas?

### Fitting a PCA

The Happy Planet Index is calculated using a weighted average of these variables. It wouldn’t make sense to try to find linear combinations with a variable that’s already a linear combination, so we remove it. 

How are the other variables related?


```{r}
happyplanet2 <- na.omit(happyplanet2016[,-8])


```

How many components should we choose?

```{r, message=FALSE}
#install.packages("factoextra")
library(factoextra)

```

Hopefully the first few PCs will account for a large percentage of the total variation.

Can also create a bi-plot - provides visualization of PCA outputs

+ Perpendicular vectors indicate the variables explain totally different types of information and are uncorrelated.
+ Two variables in the same direction are strongly positively correlated and vectors in the opposite direction are highly negatively correlated. 

```{r}

```


### `ggbiplot`

Sometimes you might want to use an R package that isn’t hosted on CRAN. One common place to host R packages is GitHub. If you know the user’s Git “repository”, you can install the package using the `install_github` function from the `devtools` library.

```{r, message=FALSE}
#install.packages('devtools')
library(devtools)

#install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(happy_pca)
```

Can use `ggbiplot` to add groups to help interpret PCs.

```{r}
ggbiplot(happy_pca, groups=happyplanet2$Region, ellipse=TRUE)
```


Can also visualize other PCs besides the first two.

```{r}
ggbiplot(happy_pca, groups=happyplanet2$Region, ellipse=TRUE, choices=c(3,4))
```



#### Try Yourself

Try the PCA with the gapminder data. Use all four variables (`infant_mortality`, `life_expectancy`, `fertility` and `popualtion`) try to make a data visualization to show the relations between the variables.

```{r}

```




### In Summary

Advantages:

- Reduced dimensions
- PCs are independent (no correlation)

Disadvantages:

- Interpretability! Sometimes PCs are easy to interpret. Other times. . . it’s a stretch at best.
