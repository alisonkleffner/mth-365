---
title: "MTH365: Introduction to Data Science"
author: "Linear Models"
date: "September 28, 2023"
output: 
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(RColorBrewer)
library(mosaic)


hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

```

## Agenda

Focus: Linear Regression

  + Quadratic Terms
  + Interaction Terms
  + Categorical Predictors
    - ANOVA
  + Assessing Variable Importance
    - Model Comparison

---
## Fit a model

Sometimes we want to know whether any variables are related, thus we need to fit a statistical model. 

__Statistical models__: 

There are two things we can do with fitting a model:
1. Interpretation 
2. Prediction

Sometimes we are only interested in one of them, sometimes both. They have slightly different approaches and may lead to different choice and explanation of the model.

Today's lecture we focus on interpretation and your mini-project 3 focuses on prediction. General steps for fitting a model for interpretation: 

1. Look at your research question. Identify the response variable.
2. Look at your data. Which variables may related to the response variables?
3. Fit the variables to a statistical model
4. Check the p-values to decide whether the variables is significant and interpret its meaning. 

---

## Looking for the potential correlation

__Example__: Review our NYC flight data. Is there any variable related to the arrival delay? If yes, how?

Consider a random sample of 1000 flights from NYC to Chicago in 2013.

```{r}
library(nycflights13)
set.seed(14)
Chicago1000 <- flights %>%
  filter(dest %in% c('ORD', 'MDW'), !is.na(arr_delay)) %>% 
  sample_n(size=1000)
```

---
## Looking for the potential correlation

Look at the column names of the data. Which variables may be related to the arrival delay?

```{r eval = FALSE}
colnames(Chicago1000)
```


```{r, echo=FALSE}
matrix(c(colnames(Chicago1000), ""), nrow = 5, ncol = 4) 
```
--

Maybe the time of the flights will affect the arrival delay. If yes, how? Let's check whether the variable `hour` is related to `arrival delay`. 

---
## Visualizing a Model

The next step is to use a model. A popular model we can use is a Linear Model.

A Linear Model is easy to visualize when looking at the relationship between two numerical variables

```{r, message=FALSE, fig.align='center', fig.height=3.5, fig.width=8}
Chicago1000 %>% 
  ggplot(aes(x = hour, y = arr_delay)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


---
## Try by yourself: 

Another variable that might impact `arr_delay` is `dep_delay`. Plot the linear model between departure delay and arrive delay in the sample

--

```{r, message=FALSE, fig.align='center', fig.height=4, fig.width=8}
Chicago1000 %>% 
  ggplot(aes(x = dep_delay, y = arr_delay)) + 
  geom_point() + 
  geom_smooth(method = "lm")

```

---
## Linear model

__Population Linear model__: The relation between the observation $Y$ and independent variables $X_1,..., X_p$ is formulated as 
$$Y = \beta_0 + \beta_1X_1 + ... + \beta_pX_p + \epsilon$$
- Y denotes the value of the response variable
- $\beta_0$ denotes the population intercept 
- $\beta_1$ denotes the population slope for the first predictor 
- $X_1$ denotes the value of the first predictor for each observation
- $\epsilon$ denotes the error

--

__Sample Linear model__: $$Y = \hat{\beta_0} + \hat{\beta_1}X_1 + ... + \hat{\beta_p}X_p$$

+ $\hat{\beta}$ denotes an estimate of the predictors
---
## Create a Linear Model

```{r}
model = lm(arr_delay ~ hour, data = Chicago1000)
summary(model)
```

---
## Create a Linear Model

Linear Model:

$$\hat{y} = -19.55 + 1.8*hours$$

**Coefficient Interpretation**: If the hour increases by one unit (one hour), then the arrival delay will increase by 1.8 units (minute). 
- Since the p value is smaller than 0.05, there is a statistical significant relationship between arrival delay and hour.
  + More on this later



---
## Adding a Quadratic Term

A population linear model with quadratic term. For example: 
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_1^2 + \epsilon$$

Typically done if you notice a curve in the data

**Note**: If you include a higher order term in the model, you must include all lower terms
  - So, if you have $X^2$, $X$ must be in the model

---
## Adding a Quadratic Term

```{r}
model2 = lm(arr_delay ~ hour + I(hour^2), data = Chicago1000)
summary(model2)
```

---
## Adding a Quadratic Term

Both terms' p values are larger than 0.05, indicating they do not have a statistical significant relation with arr_delay when both of them are added to the model. 

How do the linear and quadratic models compare?

We always start with a linear model and only use quadratic model if you have observed a potential quadratic trend in the EDA.

---
## Adding an Interaction Term

Interactions: used if you think two variables are related to each other

  - Must keep both variables in the model if interaction is significant 
  
Let's suppose we think there is an interaction between `hour` and `dep_delay`

---
### Adding an Interaction Term

```{r}
int = lm(arr_delay ~ hour + dep_delay + hour:dep_delay, 
         data = Chicago1000)
summary(int)
```


---
## Analysis of Variance (ANOVA)

What if your independent variable is categorical data?

**Analysis of variance**: used to analyze the differences among means.

--

**Example**: What about carrier? Do different carriers lead to different average arrival delay?

--

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.align='center', fig.height=5, fig.width=8}
Chicago1000 %>% 
  ggplot(aes(x = carrier, y = arr_delay)) + 
  geom_boxplot(aes(color = carrier))
```

---
## Linear Model with Categorical Explanatory

First, make sure carrier is being treated as a factor!

```{r}
Chicago1000$carrier = relevel(as.factor(Chicago1000$carrier), 
                              ref = "UA")
model3 = lm(arr_delay ~ carrier, data = Chicago1000)
broom::tidy(model3)
```

Carrier UA is the reference level and every other levels will be compared to it.

- For a categorical variable with $k$ levels, we have $k-1$ indicator variables

---
## Linear Model with Categorical Explanatory

Writing out the proper model:

$$\hat{\text{y}} = 4.86+4.35*\text{9E}-10.7*\text{AA}+20.8*\text{B6}+12.7*\text{MQ}+12.6*\text{WN}$$
--

For UA:
$$\hat{\text{y}} = 4.86 $$
--

For AA: 
$$\hat{\text{y}} = 4.86+4.35*0-10.7*1+20.8*0+12.7*0+12.6*0 = 4.86-10.7$$
$$\hat{\text{y}} = -5.84$$


---
## ANOVA Results

**Question**: Does at least one of the carriers have a different mean arrival delay than the others
```{r}
model4 = aov(arr_delay ~ carrier, data = Chicago1000)
summary(model4)
```

At least one of the carrier has different mean arrival delay than the others, thus carriers are significantly related to the arrival delay. 

---
## Statistical Significance

Null hypothesis significance testing (NHST):

`1.` Write a hypothesis

- Null Hypothesis: only randomness is in play
- Alternative Hypothesis: something else is happening

`2.` Calculate the test statistics

`3.` Achieve the p value based on the test statistics

`4.` Interpret the p values

**p-value**: Given the null hypothesis is true, the probability we observed the given data or more extreme.

--
  
**Example** For the airlines data, the null hypothesis is that there is no association between the predictors and flight delay, whereas the alternative hypothesis is that there is an association between the predictors and the flight delay.
---
## Statistical Significance

A p-values is computed by simulating a world in which a null hypothesis is set to be true (assuming no relationship, no randomness)
- Null hypothesis testing determines whether the sample results that you obtain are likely if you assume the null hypothesis is true for the population.

A very small p-value means the results are sufficiently improbable under the null hypothesis, so we reject the null hypothesis. 
- In other words, you can say that your results are statistically significant.

How small is “small enough”?
- Historically, when using hypothesis testing, analysts have declared results with a p-value of 0.05 or smaller as statistically significant, while values larger than 0.05 are declared non-significant.
- Statistical Level: A value $\alpha$ is the probability of the study rejecting the null hypothesis, given that
the null hypothesis was assumed to be true (error).

---
## Practical Significance

Something else to consider:

- Statistical significance shows that an effect exists in a study, practical
significance shows that the effect is large enough to be meaningful in the real
world
- Hypothesis tests with small differences in values can produce very low p-values if you have large sample size or the data have low variability. Hence, differences that are trivial in the practical sense can still be highly statistically significant.


---
## ANOVA: Multiple Comparisons

**From Before**: We know that at least one of the carriers has different mean arrival delay than the others, thus carriers are significant related to the arrival delay. 

```{r}
model4 = aov(arr_delay ~ carrier, data = Chicago1000)
summary(model4)
```

The ANOVA test itself provides only statistical evidence of a difference, but not any statistical evidence as to which mean or means are statistically different.

Multiple comparisons conducts an analysis of all possible pairwise means. 

An adjustment is needed to account for the number of comparisons taking place (we won't get into this). 
---
## Statistical signficance: Multiple Comparisons

```{r, eval=FALSE, message = FALSE}
library(multcomp)
model5 = glht(model4, linfct = mcp(carrier = "Tukey"))
summary(model5, test = adjusted("holm"))
```

```{r, echo=FALSE, message = FALSE}
library(multcomp)
model5 = glht(model4, linfct = mcp(carrier = "Tukey"))
s <- summary(model5, test = adjusted("holm"))
broom::tidy(s)
```

---
## Confounding

**"Correlation does not equal causation."**

What variables could be used in a model to _explain_ arrival delays?

In other words, just because there is a "statistically significant relationship" between $x$ and $y$, it doesn't mean that $x$ is _causing_ changes in $y$.

Other factors may affect (*confound*) the relationship between two variables.

---
## Carrier as a confounder?

__Example__: Does carrier confound the relationship between arrival delay and hour?

```{r, message=FALSE, fig.align='center', fig.height=4, fig.width=8}
Chicago1000 %>% ggplot(aes(x = hour, y = arr_delay)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  aes(color = carrier)
```


---
## Multiple Linear Regression

**Until Now**: We've only had one predictor variable in our model. 

But if we suspect a variable may affect the relationship between another variable and the response (or we have multiple predictor variables of interest), we need a way to test their significance.
  - Overall F Test
  - Adjusted $R^2$
  
---
## Overall F Test

Testing for significance of all predictor variables at once 
- Null: The slope for all of the predictor variables is 0
- Alternative: The slope for at least one of the predictor variables is not 0

Asking, is the regression model containing at least one of the predictors useful in predicting the response?

---
### Overall F Test

```{r, echo=FALSE}
model6 = lm(arr_delay ~ hour + carrier, data = Chicago1000)
summary(model6)
```

---
## Let's Return to the Quadratic Model

```{r}
summary(model2)
```

---
## Let's Return to the Quadratic Model

Overall F Test is significant -> at least one of the variables is useful, even though both of their individual p-values are nonsignificant

- So one of the variables is unnecessary and should be removed
  + In this case the $\text{hour}^2$ is probably unnecessary.

---
## Adjusted $R^2$

How much of the variation in our response is explained by the model

The Adjusted $R^2$ does not automatically increase for additional variables
  - If a variable is not useful, Adjusted $R^2$ will decrease 
  - Can use to help check variable importance
  
--

**Example**: 
- For our model with hour and carrier as explanatory variables, 7.361% of the variation in the response (arrival delay) is explained by the model 
- For our model with hour only as the explanatory variable, 4.071% of the variation in the response (arrival delay) is explained by the model

---
## AIC and BIC

Other methods to determine what variables to include in your model (model comparison).

They may not always agree as to which is the best model, so generally people choose the model based on which one is best among the fit statistics most often.

--

Other: Common Fit Statistics

- AIC: penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. 
```{r}
AIC(model2)
```

- BIC: Stronger penalty for including additional variables to the model.

```{r}
BIC(model2)
```


---
## Multiple Linear Regression: Coefficient Interpretations

If the hour increases by one unit (one hour), then the arrival delay will increase by 1.9055 units (minute), *holding all of the other variables constant*. 


---
## Now we can create a linear model, but is it appropriate?

1. The response has a normal distribution with constant variance for each observation
2. We need to have a linear relationship (constant variance)
  - Need to do nonlinear regression if that’s the case
3. No perfect collinearity (i.e. one predictor isn’t a linear combination of another predictor in the model)
  - Focus on numerical predictor variables
  - e.g. can’t include revenue, cost, and profit in a model because profit = revenue – cost
  
---
## Assumption: Normality

Normality: assumes that any value can be attained for this response variable
- If the response has a non-normal distribution, the estimates will be biased

To check this assumption, we use a QQ-Plot
  - A scatterplot created by plotting two sets of quantiles against one another.
  - If data is normal, we should see a roughly straight line
  
```{r, fig.align='center', fig.height=3.5, fig.width=8}
plot(model6, which = 2)
```

---
## Assumption: Linearity

Look at the Residuals vs Fitted Plot
- Look to see if there are any noticeable pattern in any of the plots
- If pattern, constant variance assumption is violated

```{r, fig.align='center', fig.height=5, fig.width=8}
plot(model6, which = 1)
```

---
## Assumption: No Perfect Collinearity

Collinearity - a linear relationship between two variables
  - one predictor is a linear combination of another predictor in the model
  - can't include revenue, profit, cost as profit = revenue-cost


High correlation between predictor variables
  - Standard errors become inflated, which can lead to $\hat{\beta}$ having the wrong sign in our predictors
  - We interpret 1 variable while holding all others constant. Holding the others constant may not be possible
  
---
## Assumption: No Perfect Collinearity

When is this an issue:
- Generally, the absolute value of a correlation above 0.8 is considered serious (caution is typically advised once you hit around 0.6)

Typically, address by removing one predictor variable at a time 
- If two variables are highly correlated, they’re both explaining the same variation in the response (redundant variable)
- Check adjusted $R^2$ values


---
## Assumption: No Perfect Collinearity

Let's look at the relationship between `air_time` and `distance`

```{r, fig.align='center', fig.height=5, fig.width=8}
library(corrplot)
M <- cor(Chicago1000[,c(15:16)])
corrplot(M, method = "number", type = "upper")
```

---
## Your Turn!

Choose some other explanatory variables and try them out! Can you find a good possible model for predicting arrival delay?
