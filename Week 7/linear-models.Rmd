---
title: "MTH365: Introduction to Data Science"
author: "Linear Models"
date: "Otober 3, 2024"
output: 
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(RColorBrewer)
library(mosaic)


hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

```

## Agenda

Focus: Linear Regression

  + Quadratic Terms
  + Interaction Terms
  + Categorical Predictors
    - ANOVA
  + Assessing Variable Importance
    - Model Comparison
    
    
    
    
---
## Language of Models

Sometimes we want to know whether any variables are related, thus we need to fit a statistical model.

- We will focus on linear models, but there are many other types of models too!

.pull-left[
```{r, message=FALSE, echo = FALSE}
x = seq(-100, 100)    # just a sequence of numbers
y = x + rnorm(length(x), 0, 50)      # generate linear association + noise

xy <- data.frame(x,y)

xy %>% ggplot(aes(x=x, y = y)) + geom_point() +
  geom_smooth(method = "lm", color = "orange", se = FALSE) +
  ggtitle("Linear Relationship")
```
].pull-right[
```{r, message=FALSE, echo = FALSE}
x = seq(-100, 100)    # just a sequence of numbers
y = x^2 + rnorm(length(x), 0, 1000)      # generate non-linear association + noise

xy <- data.frame(x,y)

xy %>% ggplot(aes(x=x, y = y)) + geom_point() +
  geom_smooth(method = "loess", color = "purple", se = FALSE) +
  geom_smooth(method = "lm", color = "orange", se = FALSE) +
  ggtitle("Non-Linear Relationship")
```

]


---
## Fit a model

Sometimes we want to know whether any variables are related, thus we need to fit a statistical model. 

__Statistical models__: 

There are two things we can do with fitting a model:
1. Interpretation 
2. Prediction

Sometimes we are only interested in one of them, sometimes both. They have slightly different approaches and may lead to different choice and explanation of the model.

Today's lecture we focus on interpretation and your mini-project 3 focuses on prediction. General steps for fitting a model for interpretation: 

1. Look at your research question. Identify the response variable.
2. Look at your data. Which variables may related to the response variables?
3. Fit the variables to a statistical model
4. Check the p-values to decide whether the variables is significant and interpret its meaning. 

---
## Benefits and Drawbacks of Models

- Models can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modeling over simple visual inspection of data.
- There is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted.

---

## Looking for the potential correlation

__Example__: Review our NYC flight data. Is there any variable related to the arrival delay? If yes, how?

Consider a random sample of 1000 flights from NYC to Chicago in 2013.

```{r, echo = FALSE}
library(nycflights13)
set.seed(14)
Chicago1000 <- flights %>%
  filter(dest %in% c('ORD', 'MDW'), !is.na(arr_delay)) %>% 
  sample_n(size=1000)
```

Look at the column names of the data. Which variables may be related to the arrival delay?

```{r eval = FALSE}
colnames(Chicago1000)
```


```{r, echo=FALSE}
matrix(c(colnames(Chicago1000), ""), nrow = 5, ncol = 4) 
```
--

Maybe the time of the flights will affect the arrival delay. If yes, how? Let's check whether the variable `hour` is related to `arrival delay`. 

---
### Visualizing a Model

The next step is to use a model. A popular model we can use is a Linear Model.

A Linear Model is easy to visualize when looking at the relationship between two numerical variables

```{r, message=FALSE, fig.align='center', fig.height=3.5, fig.width=8}
Chicago1000 %>% 
  ggplot(aes(x = hour, y = arr_delay)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


---
### Linear model

__Population Linear model__: The relation between the observation $Y$ and independent variables $X_1,..., X_p$ is formulated as 
$$Y = \beta_0 + \beta_1X_1 + ... + \beta_pX_p + \epsilon$$
- Y denotes the value of the response variable
- $\beta_0$ denotes the population intercept 
- $\beta_1$ denotes the population slope for the first predictor 
- $X_1$ denotes the value of the first predictor for each observation
- $\epsilon$ denotes the error

--

__Sample Linear model__: $$Y = \hat{\beta_0} + \hat{\beta_1}X_1 + ... + \hat{\beta_p}X_p$$

+ $\hat{\beta}$ denotes an estimate of the predictors

---
### Vocabulary

- **Response variable**: Variable whose behavior or variation you are trying to understand (y-axis)
- **Explanatory variables**: Other variables that you want to use to explain the variation in the response (x-axis)
- **Predicted value**: Output of the model function
  + The model function gives the typical (expected) value of the response variable conditioning on the explanatory variables
  
  
---
### Linear Model: One Numerical Predictor

```{r}
model = lm(arr_delay ~ hour, data = Chicago1000)
summary(model)
```

---
### Linear Model: One Numerical Predictor

Linear Model:

$$\hat{y} = -19.55 + 1.8*hours$$

**Coefficient Interpretation**: If the hour increases by one unit (one hour), then the arrival delay will increase by 1.8 units (minute). 

**Significance of Variable:** Hypothesis Test

- $H_0: \hat{\beta}_1 = 0$
- $H_0: \hat{\beta}_1 \neq 0$

  + Since the p-value (7.23e-11) is smaller than 0.05, there is a statistical significant relationship between arrival delay and hour.
  
---
### Linear Model: One Categorical Predictor

**Example**: What about carrier? Do different carriers lead to different average arrival delay?

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.align='center', fig.height=5, fig.width=8}
Chicago1000 %>% 
  ggplot(aes(x = carrier, y = arr_delay)) + 
  geom_boxplot(aes(color = carrier))
```


+ Make sure `carrier` is being treated as a factor!


---
### Linear Model: One Categorical Predictor

When a categorical explanatory variable has many levels, they're encoded to **dummy variables**
  
  - For a categorical variable with $k$ levels, we have $k-1$ indicator variables (have a baseline)
  - Each coefficient of an indicator variable describes the expected difference between that level compared to the baseline level
  
  
| carrier        | AA           | B6  | MQ| UA | WN |
| ------------- |:----:| -----:|-----:|-----:|-----:|-----:|
| 9E | 0 | 0 | 0 |  0 | 0|
| AA | 1 | 0 | 0 |  0 | 0|
| B6 | 0 | 1 | 0 |  0 | 0|
| MQ | 0 | 0 | 1 |  0 | 0|
| UA | 0 | 0 | 0 |  1 | 0|
| WN | 0 | 0 | 0 |  0 | 1|

---
### Linear Model: One Categorical Predictor

```{r}
Chicago1000$carrier = relevel(as.factor(Chicago1000$carrier), 
                              ref = "UA")
model3 = lm(arr_delay ~ carrier, data = Chicago1000)
broom::tidy(model3)
```

Carrier UA is the reference level and every other levels will be compared to it.

---
## Linear Model with Categorical Explanatory

Writing out the proper model:

$$\hat{\text{y}} = 4.86+4.35*\text{9E}-10.7*\text{AA}+20.8*\text{B6}+12.7*\text{MQ}+12.6*\text{WN}$$
--

For UA:
$$\hat{\text{y}} = 4.86 $$

Flights using Carrier UA are on average 4.86 minutes late

--

For AA: 
$$\hat{\text{y}} = 4.86+4.35*0-10.7*1+20.8*0+12.7*0+12.6*0 = 4.86-10.7$$
$$\hat{\text{y}} = -5.84$$

Flights using Carrier AA arrive, on average, 5.84 minutes earlier than flights from Carrier UA

---
## Linear Model with Categorical Explanatory

Interpreting P-Value: 

+ P-Value for `carrierAA` = 0.00468
  - $H_0: \mu_{UA} = \mu_{AA}$
  - $H_A: \mu_{UA} \neq \mu_{AA}$
+ Meaning that UA and AA have significantly different departure delays.

All p-values are comparing each category to the reference level, but what if you wanted to compare the other categories?


---
### Analysis of Variance (ANOVA)

**Analysis of variance**: used to analyze the differences among means.


**Question**: Does at least one of the carriers have a different mean arrival delay than the others

```{r}
model4 = aov(arr_delay ~ carrier, data = Chicago1000)
summary(model4)
```

At least one of the carrier has different mean arrival delay than the others (p-value = 2.6e-08), thus carriers are significantly related to the arrival delay. 

---
### ANOVA: Multiple Comparisons

**From Before**: We know that at least one of the carriers has different mean arrival delay than the others, thus carriers are significant related to the arrival delay. 

```{r}
model4 = aov(arr_delay ~ carrier, data = Chicago1000)
summary(model4)
```

The ANOVA test itself provides only statistical evidence of a difference, but not any statistical evidence as to which mean or means are statistically different.

Multiple comparisons conducts an analysis of all possible pairwise means. 

An adjustment is needed to account for the number of comparisons taking place (we won't get into this). 
---
## Statistical signficance: Multiple Comparisons

```{r, eval=FALSE, message = FALSE}
library(multcomp)
model5 = glht(model4, linfct = mcp(carrier = "Tukey"))
summary(model5, test = adjusted("holm"))
```

```{r, echo=FALSE, message = FALSE}
library(multcomp)
model5 = glht(model4, linfct = mcp(carrier = "Tukey"))
s <- summary(model5, test = adjusted("holm"))
broom::tidy(s)
```

---
## Confounding

**"Correlation does not equal causation."**

What variables could be used in a model to _explain_ arrival delays?

In other words, just because there is a "statistically significant relationship" between $x$ and $y$, it doesn't mean that $x$ is _causing_ changes in $y$.

Other factors may affect (*confound*) the relationship between two variables.

---
## Carrier as a confounder?

__Example__: Does carrier confound the relationship between arrival delay and hour?

```{r, message=FALSE, fig.align='center', fig.height=4, fig.width=8}
Chicago1000 %>% ggplot(aes(x = hour, y = arr_delay)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  aes(color = carrier)
```



---
## Multiple Linear Regression

**Until Now**: We've only had one predictor variable in our model. 

But if we suspect a variable may affect the relationship between another variable and the response (or we have multiple predictor variables of interest), we need a way to test their significance.
  - Overall F Test
  - Individual tests
  
---
## Overall F Test

Testing for significance of all predictor variables at once 
- Null: The slope for all of the predictor variables is 0
- Alternative: The slope for at least one of the predictor variables is not 0

Asking, is the regression model containing at least one of the predictors useful in predicting the response?

---
### Overall F Test

```{r, echo=FALSE}
model6 = lm(arr_delay ~ hour + carrier, data = Chicago1000)
summary(model6)
```

---
### Individual Significance Tests

Similar to model with one predictor variable, but p-value is calculated assuming all of the other variables are in the model.

- $H_0: \hat{\beta}_1 = 0 \text{; assuming carrier is included}$
- $H_0: \hat{\beta}_1 \neq 0 \text{; assuming carrier is included}$


Coefficients and p-values may change when variables are added and taken away. 

+ An increase in the hour of departure increases the arrival delay by 1.9 minutes, assume carrier is held constant

---
### Adding an Interaction Term

Interactions: used if you think two variables are related to each other

  - Allows for different slopes (coefficient for an explanatory variable would change as another explanatory variable changes)
  - Must keep both variables in the model if interaction is significant 
  
Let's suppose we think there is an interaction between `hour` and `dep_delay`

---
### Adding an Interaction Term

```{r}
int = lm(arr_delay ~ hour + dep_delay + hour:dep_delay, 
         data = Chicago1000)
summary(int)
```

---
### Variable Selection

- Adjusted $R^2$
- AIC/BIC
- Prediction Accuracy (next class period)

---
### Adjusted $R^2$

How much of the variation in our response is explained by the model

The Adjusted $R^2$ does not automatically increase for additional variables
  - If a variable is not useful, Adjusted $R^2$ will decrease 
  - Can use to help check variable importance
  
--

**Example**: 
- For our model with hour and carrier as explanatory variables, 7.361% of the variation in the response (arrival delay) is explained by the model 
- For our model with hour only as the explanatory variable, 4.071% of the variation in the response (arrival delay) is explained by the model

---
### AIC and BIC

Other methods to determine what variables to include in your model (model comparison).

They may not always agree as to which is the best model, so generally people choose the model based on which one is best among the fit statistics most often.

--

Other: Common Fit Statistics

- AIC: penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. 
```{r}
AIC(model2)
```

- BIC: Stronger penalty for including additional variables to the model.

```{r}
BIC(model2)
```


---
### Now we can create a linear model, but is it appropriate?

1. The response has a normal distribution
2. We need to have a linear relationship (constant variance)
  - Need to do nonlinear regression if that’s the case
3. No perfect collinearity (i.e. one predictor isn’t a linear combination of another predictor in the model)
  - Focus on numerical predictor variables
  - e.g. can’t include revenue, cost, and profit in a model because profit = revenue – cost
3. Observations are independent
  
---
## Assumption: Normality

Normality: assumes that any value can be attained for this response variable
- If the response has a non-normal distribution, the estimates will be biased

To check this assumption, we use a QQ-Plot
  - A scatterplot created by plotting two sets of quantiles against one another.
  - If data is normal, we should see a roughly straight line
  
```{r, fig.align='center', fig.height=3.5, fig.width=8}
plot(model6, which = 2)
```

---
## Assumption: Linearity

Look at the Residuals vs Fitted Plot
- Look to see if there are any noticeable pattern in any of the plots
- If pattern, constant variance assumption is violated

```{r, fig.align='center', fig.height=5, fig.width=8}
plot(model6, which = 1)
```

---
## Assumption: No Perfect Collinearity

Collinearity - a linear relationship between two variables
  - one predictor is a linear combination of another predictor in the model
  - can't include revenue, profit, cost as profit = revenue-cost


High correlation between predictor variables
  - Standard errors become inflated, which can lead to $\hat{\beta}$ having the wrong sign in our predictors
  - We interpret 1 variable while holding all others constant. Holding the others constant may not be possible
  
---
## Assumption: No Perfect Collinearity

When is this an issue:
- Generally, the absolute value of a correlation above 0.8 is considered serious (caution is typically advised once you hit around 0.6)

Typically, address by removing one predictor variable at a time 
- If two variables are highly correlated, they’re both explaining the same variation in the response (redundant variable)
- Check adjusted $R^2$ values


---
## Assumption: No Perfect Collinearity

Let's look at the relationship between `air_time` and `distance`

```{r, fig.align='center', fig.height=5, fig.width=8}
library(corrplot)
M <- cor(Chicago1000[,c(15:16)])
corrplot(M, method = "number", type = "upper")
```


---
### Last Thing

- Non-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.
  + Common transformation is the log-transformation or square root
  + Be careful with your interpretations (untransform your response variable)
- If you notice a curve in your data, you can add a quadratic term
  + $Y = \hat{\beta_0} + \hat{\beta_1}X_1 + \hat{\beta_2}X_1^2$
  + `lm(arr_delay ~ hour + I(hour^2), data = Chicago1000)`
- When writing about significant variables, include their p-value and standard error in the write-up


---
## Your Turn!

Choose some other explanatory variables and try them out! Can you find a good possible model for predicting arrival delay?


