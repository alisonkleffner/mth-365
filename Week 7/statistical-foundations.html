<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MTH365: Introduction to Data Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Statistical Inference" />
    <meta name="date" content="2024-10-04" />
    <script src="libs/header-attrs-2.27/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# MTH365: Introduction to Data Science
]
.author[
### Statistical Inference
]
.date[
### October 4, 2024
]

---




## Agenda

- Population vs. Sampling
- Sampling Distributions
  + Standard Error
- Confidence Intervals
- Bootstrapping 

---
## Announcements




---
## Isn't this data science?

- Statistics: Statistics is a mathematically based field which seeks to collect and interpret quantitative data. 

- Data science: Data science is a multidisciplinary filed which uses scientific methods, processes and system to extract information from data in a large range of forms. 

Data science techniques can provide us a clean data and its visualization but statistical theory is still useful to figure out what the data is tell us. 

---
## Introduction

Statistical Inference: the process of using sample data to make conclusions about the underlying population the sample came from

Before we talk about methods for statistical inference, let's discuss uncertainty
- Not only should report the point estimate, but should also quantify our uncertainty

--

What do we mean by uncertainty?

- Roughly equivalent to the notion of repeatability. 
  + You’re certain if your results are repeatable. 
  + You’re uncertain if your results are subject to a lot of statistical fluctuations.

---
## Why are Statisticians Always Uncertain?


- Our data consists of a sample
- We want to use our models to make a prediction, but there are always things that we can't account for in a model
- Our observations are subject to measurement or reporting error
- Our data arise from an intrinsically random or variable process

---
## Samples and Population

In statistics we are interested in a **population** of cases/people/objects. However, often this population is too large to collect data on, so we take **samples** from the larger population.

Statistical methodology assumes that the cases are drawn from a much larger set of potential cases, so the given data are a sample of a larger population of potential cases. 
  - Other samples that might have been drawn from the population.

--

**Next**:
- Why sample?
- Requirements of a “good” sample?


---
## Example: Setting travel policy by sampling from the population

Example: You’ve been asked to develop a travel policy for business travelers going from New York City to Chicago. Assume that the `nycflights13` data sets represents the complete population of flights.

What would we do?

Let’s first filter all the flights going to Chicago and with a non-NA value of arrival delay time.


``` r
library(nycflights13)
Chicago &lt;- flights %&gt;%
  filter(dest %in% c('ORD', 'MDW'), !is.na(arr_delay))
```

--

Let's take a sample (n=100)!


``` r
set.seed(365)
Sample100 &lt;- Chicago %&gt;% sample_n(size=100)
```


---
### Let's find a Sample!


``` r
glimpse(Sample100)
```

```
## Rows: 100
## Columns: 19
## $ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013…
## $ month          &lt;int&gt; 9, 12, 7, 10, 10, 6, 4, 4, 6, 9, 4, 11, 10, 9, 6, 1, 2, 2, 4, 7, 11, 4, 3, 1, 12, 9, 12,…
## $ day            &lt;int&gt; 15, 18, 2, 10, 7, 24, 28, 18, 2, 8, 25, 14, 29, 8, 20, 15, 5, 11, 22, 20, 8, 9, 20, 21, …
## $ dep_time       &lt;int&gt; 603, 2034, 1800, 629, 2116, 1156, 1759, 906, 1157, 1325, 1222, 1121, 1156, 2049, 2113, 1…
## $ sched_dep_time &lt;int&gt; 601, 2035, 1805, 630, 1901, 1200, 1800, 630, 1200, 1330, 1225, 1125, 1200, 2030, 2100, 1…
## $ dep_delay      &lt;dbl&gt; 2, -1, -5, -1, 135, -4, -1, 156, -3, -5, -3, -4, -4, 19, 13, 26, -4, 1, 17, -5, -8, 79, …
## $ arr_time       &lt;int&gt; 717, 2153, 1926, 737, 2226, 1300, 1910, 1244, 1303, 1452, 1438, 1245, 1308, 2148, 2218, …
## $ sched_arr_time &lt;int&gt; 730, 2210, 1950, 805, 2040, 1333, 1944, 804, 1310, 1510, 1405, 1310, 1315, 2150, 2235, 1…
## $ arr_delay      &lt;dbl&gt; -13, -17, -24, -28, 106, -33, -34, 280, -7, -18, 33, -25, -7, -2, -17, 39, 29, 11, 20, 8…
## $ carrier        &lt;chr&gt; "UA", "WN", "AA", "AA", "UA", "UA", "UA", "B6", "WN", "AA", "AA", "AA", "WN", "WN", "AA"…
## $ flight         &lt;int&gt; 1143, 168, 353, 303, 693, 255, 994, 905, 654, 331, 329, 327, 3330, 565, 371, 464, 905, 1…
## $ tailnum        &lt;chr&gt; "N68453", "N930WN", "N3CPAA", "N489AA", "N429UA", "N824UA", "N808UA", "N296JB", "N425LV"…
## $ origin         &lt;chr&gt; "EWR", "LGA", "LGA", "LGA", "LGA", "LGA", "EWR", "JFK", "EWR", "LGA", "LGA", "LGA", "EWR…
## $ dest           &lt;chr&gt; "ORD", "MDW", "ORD", "ORD", "ORD", "ORD", "ORD", "ORD", "MDW", "ORD", "ORD", "ORD", "MDW…
## $ air_time       &lt;dbl&gt; 116, 116, 117, 105, 111, 106, 104, 168, 110, 114, 116, 119, 119, 99, 107, 133, 129, 133,…
## $ distance       &lt;dbl&gt; 719, 725, 733, 733, 733, 733, 719, 740, 711, 733, 733, 733, 711, 711, 733, 719, 740, 719…
## $ hour           &lt;dbl&gt; 6, 20, 18, 6, 19, 12, 18, 6, 12, 13, 12, 11, 12, 20, 21, 16, 6, 7, 15, 9, 6, 9, 16, 17, …
## $ minute         &lt;dbl&gt; 1, 35, 5, 30, 1, 0, 0, 30, 0, 30, 25, 25, 0, 30, 0, 50, 30, 0, 0, 35, 30, 55, 0, 0, 20, …
## $ time_hour      &lt;dttm&gt; 2013-09-15 06:00:00, 2013-12-18 20:00:00, 2013-07-02 18:00:00, 2013-10-10 06:00:00, 201…
```

---
## Finding Summary Statistics

How long of a delay should we expect based on the sample? We can look at the mean, minimum, maximum, and standard deviation to get some idea.


``` r
Sample100 %&gt;% summarize(mean=mean(arr_delay),
                        min=min(arr_delay),
                        max=max(arr_delay), 
                        sd=sd(arr_delay))
```

```
## # A tibble: 1 × 4
##    mean   min   max    sd
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  8.96   -39   280  46.5
```

---
### Finding Summary Statistics

We might also want to examine the quantiles.



``` r
Sample100 %&gt;% summarize(q05=quantile(arr_delay, 0.05), 
                        q25=quantile(arr_delay, 0.25),
                        median=median(arr_delay),
                        q75=quantile(arr_delay, 0.75), 
                        q95=quantile(arr_delay, 0.95))
```

```
## # A tibble: 1 × 5
##     q05   q25 median   q75   q95
##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1   -33   -18   -6.5  17.2  102.
```

Sorting the values from min to max, Quantile X means there are x% of values less than it:

  - ie. 75% of the values are less than 17.2
  
---
# What constitutes an “unacceptable” delay?

How about an one and half hour delay?


``` r
Chicago %&gt;% mutate(less90 = arr_delay&lt;=90) %&gt;% 
  group_by(less90) %&gt;% 
  summarize(n=n())
```

```
## # A tibble: 2 × 2
##   less90     n
##   &lt;lgl&gt;  &lt;int&gt;
## 1 FALSE   1197
## 2 TRUE   19394
```


---
## Take Another sample



``` r
set.seed(10)
Sample100_2 &lt;- Chicago %&gt;% sample_n(size=100)

Sample100_2 %&gt;% summarize(mean=mean(arr_delay),
            min=min(arr_delay),
            max=max(arr_delay), 
            sd=sd(arr_delay))
```

```
## # A tibble: 1 × 4
##    mean   min   max    sd
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  9.64   -42   175  46.4
```


The sample mean is now 9.64. Is this the same or different than the previous sample?

--

Different samples of data will have different sample statistics!

---
## Compare to Population to Check Sample


``` r
Sample100_2 %&gt;% summarize(mean=mean(arr_delay), #sample
            min=min(arr_delay),
            max=max(arr_delay), 
            sd=sd(arr_delay)) 
```

```
## # A tibble: 1 × 4
##    mean   min   max    sd
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  9.64   -42   175  46.4
```

``` r
Chicago %&gt;% summarize(mean=mean(arr_delay), #population
            min=min(arr_delay),
            max=max(arr_delay), 
            sd=sd(arr_delay))
```

```
## # A tibble: 1 × 4
##    mean   min   max    sd
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  7.14   -62  1109  47.9
```

--
However, in most real-world settings, we do not have access to the population data. We have only our sample.

---
## Sample Statistics

Statistic: A quantity computed from values in a sample to represent some characteristics

Examples:

+ Mean `\((\bar{x})\)`
+ Proportion `\((\hat{p})\)`
+ Median `\((m)\)`
+ Standard Deviation `\((s)\)`

These are also called **point estimates**: single value from a sample to approximate this value in the population.

---
## Sampling Distributions

Sample statistics depend completely on the data. Different samples of data will have different sample statistics!

We need to figure out the reliability of a sample statistic from the sample itself. 
  + For now, though, we are going to use the population to develop some ideas about how to define reliability. 

--

If we draw many different samples from the population, each of size `\(n\)`, and calculated the sample statistic on each of those samples, how similar would the sample statistic be across all the samples?
  + Do so by creating a sampling distribution

--

**Sampling distribution**: a set of probabilities represents the chance to see certain values of the statistics
+ [Animation](https://onlinestatbook.com/stat_sim/sampling_dist/index.html)

---
## Creating a Sampling Distribution

We could run the previous code multiple times to obtain our different samples

--

Or, we could speed it up:


``` r
n &lt;- 100
num_trials &lt;- 500
chi_25_means &lt;- 1:num_trials %&gt;% 
  map(~Chicago %&gt;% slice_sample(n = n) %&gt;% 
        summarize(mean_arr_delay = mean(arr_delay))) %&gt;% 
  list_rbind() %&gt;% 
  mutate(n = n)

head(chi_25_means)
```

```
## # A tibble: 6 × 2
##   mean_arr_delay     n
##            &lt;dbl&gt; &lt;dbl&gt;
## 1          10.4    100
## 2           9.98   100
## 3           5.49   100
## 4           2.16   100
## 5          12.1    100
## 6          15.5    100
```

---
## Creating a Sampling Distribution

``` r
# favstats() is in the mosaic library
favstats(~mean_arr_delay, data=chi_25_means)
```

```
##    min     Q1 median      Q3   max   mean       sd   n missing
##  -4.19 3.2275   6.64 10.3025 21.15 7.0059 4.808579 500       0
```

--


``` r
ggplot(chi_25_means, aes(x=mean_arr_delay)) + 
  geom_density(fill='turquoise', alpha=0.5) + 
  labs(x='Sampling Distribution of Sample Mean')
```

&lt;img src="statistical-foundations_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---
## What does the Sampling Distribution tells us?

- Where is the sampling distribution centered? 
  + The expected value of a statistical summary is the average of that summary’s sampling distribution—that is, the average value of that summary under repeated sampling from the same random process that generated our data.

- How spread out is the sampling distribution? 
  + The answer to this question provides a quantitative measure measure of repeatability, and therefore statistical uncertainty.
  + The standard deviation of a sampling distribution is called the **standard error**. This reflects the typical statistical fluctuation of our summary statistic

---
## Discuss Reliability: Standard Error

**Standard error**: It describes the width of the sampling distribution.

`$$SE = \frac{s}{\sqrt{n}}$$`
--

Example: What’s the standard error of the sample mean?


``` r
favstats(~arr_delay, data=Sample100)
```

```
##  min  Q1 median    Q3 max mean       sd   n missing
##  -39 -18   -6.5 17.25 280 8.96 46.46665 100       0
```


``` r
favstats(~mean_arr_delay, data=chi_25_means)
```

```
##    min     Q1 median      Q3   max   mean       sd   n missing
##  -4.19 3.2275   6.64 10.3025 21.15 7.0059 4.808579 500       0
```


---
## Discuss Repeatability: Confidence Intervals

Now suppose we want to report a range of plausible values that have a good shot at capturing the parameter instead of just the point estimate
- Quantifying this requires a measurement of how much we would expect the sample population to vary from sample to sample
- It is calculated from the mean and standard error of the sampling distribution.

--

**Approximate 95% Confidence interval**: If generate repeated samples, 95% of those intervals will contain the true population value.


&lt;img src="../Week 7/images/confidence-interval.png" width="70%" style="display: block; margin: auto;" /&gt;


---
## Discuss Repeatability: Confidence Intervals

Example: Calculate and interpret an approximate 95% confidence interval for the mean arrival delay.

`$$\bar{x} \pm 2*SE$$`


``` r
chi_25_means %&gt;%
  summarize(
    x_bar = mean(mean_arr_delay),
    se = sd(mean_arr_delay)
  ) %&gt;%
  mutate(
    ci_lower = x_bar - 2 * se, # approximately 95% of observations 
    ci_upper = x_bar + 2 * se  # are within two standard errors
  )
```

```
## # A tibble: 1 × 4
##   x_bar    se ci_lower ci_upper
##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1  7.01  4.81    -2.61     16.6
```

---
## Discuss Repeatability: Approximate 95% confidence interval


``` r
ggplot(chi_25_means, aes(x=mean_arr_delay)) + 
  geom_density(fill='turquoise', alpha=0.5) + 
  labs(x='Sampling Distribution of Sample Mean') +
* geom_vline(xintercept=c(-2.61, 16.6), color="red")
```

&lt;img src="statistical-foundations_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

---
## How does sample size affect the sampling distribution?


```r
Means50 &lt;- do(1000)*(Chicago %&gt;% sample_n(size=50) %&gt;% 
                       summarize(mean=mean(arr_delay)))
Means100 &lt;- do(1000)*(Chicago %&gt;% sample_n(size=100) %&gt;% 
                        summarize(mean=mean(arr_delay)))
Means500 &lt;- do(1000)*(Chicago %&gt;% sample_n(size=500) %&gt;% 
                        summarize(mean=mean(arr_delay)))
```



&lt;img src="statistical-foundations_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;


---
## How does sample size affect the sampling distribution?


A larger sample size produces a smaller standard error. 
  - A larger sample size is more reliable than a smaller sample size

--

What happens to the confidence interval as the standard error decreases?

--

+ Smaller Interval

--

For large sample sizes, the shape of the sampling distribution tends to bell-shaped


---
## Bootstraping

In the last example, we had access to the population data and so we could find the sampling distribution by repeatedly sampling from the population. 
  - We typically only have one sample and not the entire population. 

--

**Bootstrap** is a method that allows us to approximate the sampling distribution even though we do not have the population.

In bootstraping we think of our sample as if it were the population. 
  - Still drawing many new samples from our original sample (**Resampling**). 

When resampling, we sample with replacement.
  - Allows us to estimate the variability of the sample
  - Otherwise, would always get the same sample statistics as the observed value.

Bootstrapping does not create new cases (not for data collection)

---
## Bootstraping

&lt;img src="../Week 7/images/bootstrap.png" width="2107" style="display: block; margin: auto;" /&gt;

  
---
## Bootstraping: Let's take a small sample (n=3)


``` r
f3 &lt;- Chicago %&gt;% sample_n(size=3) %&gt;% dplyr::select(year,month,day)
```


```
## # A tibble: 3 × 3
##    year month   day
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1  2013    10    14
## 2  2013    12    31
## 3  2013     1     2
```

--

.pull-left[
First Resample: 

``` r
f3 %&gt;% slice_sample(n= 3, 
               replace = TRUE)
```

```
## # A tibble: 3 × 3
##    year month   day
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1  2013    12    31
## 2  2013    12    31
## 3  2013    10    14
```
].pull-right[
Second Resample: 

``` r
f3 %&gt;% slice_sample(n= 3, 
               replace = TRUE)
```

```
## # A tibble: 3 × 3
##    year month   day
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1  2013     1     2
## 2  2013    12    31
## 3  2013    12    31
```
]

---
## Bootstrapping: Bigger Sample


``` r
Bootstrap_Means &lt;- Sample100 %&gt;% 
  specify(response = arr_delay) %&gt;%
  generate(reps = 500, type = "bootstrap") %&gt;%
  calculate(stat = "mean")

ggplot(Bootstrap_Means, aes(x=stat))+
  geom_density(fill='turquoise', alpha=0.5)+labs(x='Bootstrap Means')
```

&lt;img src="statistical-foundations_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;

---
## Bootstrapping: Bigger Sample


Bootstrap Sample:

``` r
favstats(~stat, data=Bootstrap_Means)
```

```
##    min   Q1 median     Q3   max    mean       sd   n missing
##  -5.22 5.55  8.725 12.295 23.57 8.95738 4.739031 500       0
```

Sampling Distribution:

``` r
favstats(~mean_arr_delay, data=chi_25_means)
```

```
##    min     Q1 median      Q3   max   mean       sd   n missing
##  -4.19 3.2275   6.64 10.3025 21.15 7.0059 4.808579 500       0
```

--

For moderate to large sample sizes and sufficient number of bootstraps, the bootstrap distribution approximates certain aspects of the sampling distribution, like the standard error and quantiles 

---
## Comparison of 3 Bootstrap Distributions

&lt;img src="statistical-foundations_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;

---
## Side Note: We want Quality Samples!

The quality of bootstrap estimates depends on the quality of the collected data.
  - So we want quality samples!
  
Quality samples tend to be representative of the population of interest

--

Is this class a quality sample of all Creighton students?

---
## Outliers

Outliers: a data point that differs significantly from other observations

When you have an outlier:

- Identify the potential outlier
- Try to understand why it is an outlier
- Only remove it if you are sure it is caused by completely random error, or not related to your research focus.

---
## Hypothesis Tests

Used to test an assumption about a population parameter

Made up of 4 steps:

1. Start with a null hypothesis `\((H_0)\)`, that represents the status quo ("there is nothign going on")

2. Set an alternative hypothesis `\((H_A)\)`, that represents the research question, i.e. what we’re testing for
  - Can be one-sided or two-sided

3. Conduct a hypothesis test under the assumption that the null hypothesis is true
  - calculate a test-statistic/p-value 

4. If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis
  - If they do, then reject the null hypothesis in favor of the alternative


---
### Example: New England Patriots

The Patriots’ fan base is hugely devoted, probably due to their long run of success over nearly two decades. Many others, however, dislike the Patriots for their highly publicized cheating episodes, whether for deflating footballs or filming the practice sessions of their opponents. But did the Patriots cheat at the pre-game coin toss, which decides who starts the game with the ball?

Believe it or not, many people think so. That’s because, for a stretch of 25 games spanning the 2014-15 NFL seasons, the Patriots won 19 out of 25 coin tosses, for a suspiciously high 76% winning percentage. For the Patriots’ detractors, this fact was clear evidence that something fishy was going on. 

But before turning to fraud or the Force as an explanation, let’s take a closer look at the evidence. Just how likely is it that one team could win the pre-game coin toss at least 19 out of 25 times, assuming that there’s no cheating going on? We are going to examine this using a hypothesis test



--

`\(H_0:\)`  Pre-game coin toss in the Patriots’ games was truly random during the 2014-2015 NFL seasons.

`\(H_A:\)` Patriots won the pre-game coin toss more often than what would be expected by random chance during the 2014-2015 NFL seasons.


---
### New England Patriots: Null Distribution

Our burning question: just how likely is it that one team could win the pre-game coin toss at least 19 out of 25 times, assuming that there’s no cheating going on? We’d “expect” any given team to win only about half of its pre-game coin tosses, but we also have to account for the possibility that they just got lucky. Hence we want to simulate a random process, assuming the null hypothesis is true, called a **Null Distribution**


&lt;img src="statistical-foundations_files/figure-html/unnamed-chunk-29-1.png" style="display: block; margin: auto;" /&gt;


---
### New England Patriots: P-value

Test Statistic:

``` r
sum(patriots_sim &gt;= 19)
```

```
## [1] 83
```

--
A **p-value** is the probability of observing a test statistic as extreme, or more extreme, than the test statistic observed, given that the null hypothesis is true.

&lt;img src="statistical-foundations_files/figure-html/unnamed-chunk-31-1.png" style="display: block; margin: auto;" /&gt;



---
### Making Conclusions

We often use 5% as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model. This cutoff value is called the significance level, `\(\alpha\)`.

- If p-value &lt; `\(\alpha\)` , reject `\(H_0\)` in favor of `\(H_A\)`: The data provide convincing evidence for the alternative hypothesis.

- If p-value &gt; `\(\alpha\)`, fail to reject `\(H_0\)` in favor of `\(H_A\)`: The data do not provide convincing evidence for the alternative hypothesis.

--

In our example, since 0.0083 &lt; 0.05, we have evidence that the New England Patriots won the coin toss more than what would be expected by random chance during the 2014-2015 NFL season.


---
## P-Values

Using a p-value as a measure of statistical significance has both advantages and disadvantages:
- The main advantage is that the p-value gives us a continuous measure of evidence against the null hypothesis. 
  + The smaller the p-value, the more unlikely it is that we would have seen our data under the null hypothesis, and therefore the stronger (“more significant”) the evidence that the null hypothesis is false.
- The main disadvantage is that the p-value is really, really hard to interpret correctly.

---
### P-Value Controversy

To avoid thinking too hard about what a p-value actually means, many people just shrug their shoulders and ask a simple question about any particular p-value they happen to encounter: is it less than some magic threshold? 
- Of course, there is no such thing as a genuinely magic threshold where results become important, any more than there is such a thing as a magic height at which you can suddenly play professional basketball.

Need to distinguish between statistical significance and practical significance.

- Statistical significance just means whether we reject `\(H_0\)`
- Practical significance, means whether the numerical magnitude of something is large enough to matter to actual human persons
  + Hypothesis tests with small differences in values can produce very low p-values if you have large sample size or the data have low variability.


---
### New England Patriots: Conclusion

But despite the small probability of such an extreme result, it’s hard to believe that the Patriots cheated on the coin toss, for a few reasons. 

--
  - How? The coin toss would be extremely hard to manipulate, even if you were inclined to do so. 
  - The Patriots are just one team, and this is just one 25-game stretch. But the NFL has 32 teams, so the probability that at least one of them would go on an unusual coin-toss winning streak over at least one 25-game stretch over a long time period is a lot larger than the number we’ve calculated. 
  - After this 25-game stretch, the Patriots reverted back to a more typical coin-toss winning percentage, closer to 50%. 

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
