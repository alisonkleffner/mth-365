---
title: "DSC365: Introduction to Data Science"
author: "Statistical Foundations"
date: "October 2, 2025"
output: 
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(RColorBrewer)
library(mosaic)
library(infer)


hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

```

### Announcements

**Lab 4**

- Due Thursday October 9th, 2025 at 11:59 pm in Blueline

**Quiz 2**

- Last 30 minutes of class Thursday October 9th
- Topics: Data Communication and Statistical Foundations

**Lab 5**: work day in class on Thursday October 9th

- Due Tuesday October 21st, 2025 at 11:59 pm in Blueline

**Mini Project 2**

- Due Thursday October 23rd at 11:59 in Blueline




---
### Isn't this data science?

- Statistics: Statistics is a mathematically based field which seeks to collect and interpret quantitative data. 

- Data science: Data science is a multidisciplinary filed which uses scientific methods, processes and system to extract information from data in a large range of forms. 


---
### Samples and Population

In statistics we are interested in a **population** of cases/people/objects. However, often this population is too large to collect data on, so we take **samples** from the larger population.

Statistical methodology assumes that the cases are drawn from a much larger set of potential cases, so the given data are a sample of a larger population of potential cases. 

---
### Uncertainty

What do we mean by uncertainty?

- Roughly equivalent to the notion of repeatability. 
  + You’re certain if your results are repeatable. 
  + You’re uncertain if your results are subject to a lot of statistical fluctuations.
  
<br>
<br>

Why are Statisticians Always Uncertain?
 
- Our data consists of a sample
- We want to use our models, but there are always things that we can't account for in a model
- Our observations are subject to measurement or reporting error
- Our data arise from an intrinsically random or variable process

---
### Example: Setting travel policy through sampling

**Example**: You’ve been asked to develop a travel policy for business travelers going from New York City to Chicago. Assume that the `nycflights13` data set represents the complete population of flights.


**Clean Data:** filter all the flights going to Chicago and with a non-NA value of arrival delay time.

```{r}
library(nycflights13)
Chicago <- flights %>%
  filter(dest %in% c('ORD', 'MDW'), !is.na(arr_delay))

mean(Chicago$arr_delay) # population value
```

---
### Let's find a Sample (n=100)!

How long of a delay should we expect based on the sample?

```{r}
set.seed(365)
Sample100 <- Chicago %>% sample_n(size=100)
```


```{r}
Sample100 %>% summarize(min=min(arr_delay),
                        q05=quantile(arr_delay, 0.05),
                        mean=mean(arr_delay),
                        median=median(arr_delay),
                        max=max(arr_delay), 
                        q95=quantile(arr_delay, 0.95),
                        sd=sd(arr_delay))
```

---
### What constitutes an “unacceptable” delay?

How about an one and forty-five minuted delay (105 minutes)?

```{r}
Chicago %>% mutate(less105 = arr_delay<=105) %>% 
  group_by(less105) %>% 
  count() %>%
  mutate(pct = n / nrow(Chicago))
```

---
### Take Another sample


```{r}
set.seed(10)
Sample100_2 <- Chicago %>% sample_n(size=100)

Sample100_2 %>% summarize(mean=mean(arr_delay),
            min=min(arr_delay),
            max=max(arr_delay), 
            sd=sd(arr_delay))
```


The sample mean is now 9.64. How does this compare to the previous sample? 

---
### Sample Statistics 

Statistic: A quantity computed from values in a sample to represent some characteristics

Examples:

+ Mean $(\bar{x})$
+ Proportion $(\hat{p})$
+ Standard Deviation $(s)$

---
### Sampling Distributions

Sample statistics depend completely on the data. Different samples of data will have different sample statistics!

We need to figure out the reliability of a sample statistic from the sample itself. 
  + For now, though, we are going to use the population to develop some ideas about how to define reliability. 


---
### Creating a Sampling Distribution

We could run the previous code multiple times to obtain our different samples or, we could speed it up:

```{r}
n <- 100
num_trials <- 500
chi_25_means <- 1:num_trials %>% 
    map_dfr(~Chicago %>% slice_sample(n = n) %>% 
        summarize(mean_arr_delay = mean(arr_delay))) %>% 
  mutate(n = n)

head(chi_25_means)
```

---
### Creating a Sampling Distribution

```{r}
favstats(~mean_arr_delay, data=chi_25_means) #mosaic library
```

```{r, fig.height=4.5, fig.width=8, fig.align='center', fig.alt="Density plot depicting the approximate sampling distribution of the sample mean. It is an approximate bell-shaped curve."}
ggplot(chi_25_means, aes(x=mean_arr_delay)) + 
  geom_density(fill='turquoise', alpha=0.5) + 
  labs(x='Approximate Sampling Distribution of Sample Mean')
```

---
### What does the Sampling Distribution tells us?

- Where is the sampling distribution centered? 
  + The expected value of a statistical summary is the average of that summary’s sampling distribution—that is, the average value of that summary under repeated sampling from the same random process that generated our data.

- How spread out is the sampling distribution? 
  + The answer to this question provides a quantitative measure of repeatability, and therefore statistical uncertainty.
  + The standard deviation of a sampling distribution is called the **standard error**. This reflects the typical statistical fluctuation of our summary statistic

---
### Discuss Reliability: Standard Error

**Standard error**: It describes the width of the sampling distribution.

$$SE = \frac{s}{\sqrt{n}}$$

Example: What’s the standard error of the sample mean?

```{r}
favstats(~arr_delay, data=Sample100)
```

```{r}
favstats(~mean_arr_delay, data=chi_25_means)
```


---
### How does n Affect the Sampling Distribution?

```{r means, message=FALSE, warning=FALSE, cache=TRUE}
Means50 <- do(1000)*(Chicago %>% sample_n(size=50) %>% 
                       summarize(mean=mean(arr_delay)))
Means100 <- do(1000)*(Chicago %>% sample_n(size=100) %>% 
                        summarize(mean=mean(arr_delay)))
Means500 <- do(1000)*(Chicago %>% sample_n(size=500) %>% 
                        summarize(mean=mean(arr_delay)))
```

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=8, echo=FALSE}
Means <- rbind(Means50 %>% mutate(n=50),
               Means100 %>% mutate(n=100),
               Means500 %>% mutate(n=500))
```

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=8, echo=FALSE, fig.alt="This figure shows the impact of sample size on sampling distributions. As the sample size increases from 50 to 500, the width or variability of the sampling distribution becomes narrower."}
ggplot(dat=Means, aes(x=mean)) + 
  geom_density(aes(fill=as.factor(n))) + 
  facet_grid(~n)+xlab('Sample means') + guides(fill=FALSE)
```


---
### Bootstraping

In the last example, we had access to the population data and so we could find the sampling distribution by repeatedly sampling from the population. 

**Bootstrap** is a method that allows us to approximate the sampling distribution even though we do not have the population.

In bootstraping we think of our sample as if it were the population. 
  - Still drawing many new samples from our original sample (**Resampling**). 

When resampling, we sample with replacement.
  - Allows us to estimate the variability of the sample
  - Otherwise, would always get the same sample statistics as the observed value.

---
### Bootstraping


---
### Bootstraping: Let's take a small sample (n=3)

```{r}
f3 <- Chicago %>% sample_n(size=3) %>% dplyr::select(year,month,day)
```

```{r, echo=FALSE}
f3 
```


.pull-left[
First Resample: 
```{r}
f3 %>% slice_sample(n= 3, 
               replace = TRUE)
```
].pull-right[
Second Resample: 
```{r}
f3 %>% slice_sample(n= 3, 
               replace = TRUE)
```
]

---
### Bootstrapping: Bigger Sample

```{r, fig.height=4.5, fig.width=8, fig.align='center', fig.alt="Density plot of the bootstraped distribution for sample means. This is approximately normal and is similar in appearance to the sampling distribution."}

Bootstrap_Means <- Sample100 %>% 
  specify(response = arr_delay) %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "mean")

ggplot(Bootstrap_Means, aes(x=stat))+
  geom_density(fill='turquoise', alpha=0.5)+labs(x='Bootstrap Means')

```

---
### Bootstrapping: Bigger Sample


Bootstrap Sample:
```{r}
favstats(~stat, data=Bootstrap_Means)
```

Sampling Distribution:
```{r}
favstats(~mean_arr_delay, data=chi_25_means)
```

---
### Side Note: We want Quality Samples!

The quality of bootstrap estimates depends on the quality of the collected data.
  - So we want quality samples!
  
Quality samples tend to be representative of the population of interest


---
### Outliers

Outliers: a data point that differs significantly from other observations

When you have an outlier:

- Identify the potential outlier
- Try to understand why it is an outlier
- Only remove it if you are sure it is caused by completely random error, or not related to your research focus.

---
### Confounding Variables and Simpson's Paradox

.center[
Correlation does not imply causation
]

<br>

- A common concern with observational data is that correlations are distorted by other variables -> *confounding variables*
  + Randomized trials control for this; but not always practical
  
- In observational studies, addressing confounding is straightforward if these variables are measured (ex. can add them to model)


---
### Confounding Variables and Simpson's Paradox


Simpson's paradox is a phenomenon where a trend appears in several groups of data but disappears or reverses when the groups are combined. 
- The paradox can be resolved when confounding variables are appropriately addressed, but can lead to problematic interpretations if they are not. 

```{r, echo=FALSE, fig.align='center', out.height="90%", out.width="85%", fig.alt="Graphic depicting an example of simpsons paradox. It shows that the overall trend between height and basketball success is negative, which seems suspicious. However, when we add a third variable of age we see the reason for this trend as the data points for ages are clumped together."}

knitr::include_graphics("~/Documents/Classes/MTH365/mth-365/08-statistical-foundations/slides/images/simpson-paradox.png")

```


---
### Hypothesis Tests and its Perils

Made up of 4 steps:

1). Write out your hypotheses for testing

- Start with a null hypothesis $(H_0)$, that represents the status quo ("there is nothign going on")
- Set an alternative hypothesis $(H_A)$, that represents the research question, i.e. what we’re testing for
  - Can be one-sided $(<,>)$ or two-sided $(\neq)$

2/3). Conduct a hypothesis test under the assumption that the null hypothesis is true
  - calculate a test-statistic/p-value 

4). If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis
  - If they do, then reject the null hypothesis in favor of the alternative


---
### Example: Lee Corso Head Gear Picks

Lee Corso is a football analyst and broadcaster commonly known for his “headgear” picks on ESPNs College GameDay until his retirement earlier this season. But the real question is, does he know what he is talking about? We randomly selected 100 games and found he correctly picked 62 teams correctly out of 100 games (62%)

```{r, echo=FALSE, fig.align='center', out.width="40%"}

knitr::include_graphics("~/Documents/Classes/MTH365/mth-365/08-statistical-foundations/slides/images/corso.png")

```


$H_0:$  Corso's picks for the winning football team was truly random during his career.

$H_A:$ Corso's picks for the winning football team was higher than what would be expected by random chance throughout his career.


---
### Null Distribution

If he was just guessing, we'd expect him to correctly pick the correct team about half of the time, but we also have to account for the possibility that they just got lucky. Hence we want to simulate a random process, assuming the null hypothesis is true, called a **Null Distribution**

<br>

```{r, echo = FALSE, fig.align='center', fig.width=10, fig.height=5}
set.seed(4)
patriots_sim = do(10000)*nflip(100)

ggplot(patriots_sim) + 
  geom_histogram(aes(x=nflip), binwidth=1)
```


---
### New England Patriots: P-value

A **p-value** is the probability of observing a test statistic as extreme, or more extreme, than what observed, given that the null hypothesis is true.

<br>

```{r, echo = FALSE, fig.align='center', fig.width=10, fig.height=5}
#sum(patriots_sim >= 19)

patriots_sim <- patriots_sim %>% mutate(pvalue = ifelse(nflip < 62, "less", "greater"))

ggplot(patriots_sim) + 
  geom_histogram(aes(x=nflip, fill = pvalue), binwidth=1) +
  geom_vline(xintercept = 61.5) + 
  geom_text(x=64, y=500, label='P(>= 62 Wins) = 0.0111', colour='black', size=5) + theme(legend.position="none")


```


---
### Making Conclusions

We often use 5% as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model. This cutoff value is called the significance level, $\alpha$.

- If p-value < $\alpha$ , reject $H_0$ in favor of $H_A$: The data provide convincing evidence for the alternative hypothesis.

- If p-value > $\alpha$, fail to reject $H_0$ in favor of $H_A$: The data do not provide convincing evidence for the alternative hypothesis.

<br>

In our example, since 0.0111 < 0.05, we have evidence that Corso's picks for the winning football team was higher than what would be expected by random chance throughout his career.

---
### P-Values

Using a p-value as a measure of statistical significance has both advantages and disadvantages:
- **Advantage** is that the p-value gives us a continuous measure of evidence against the null hypothesis. 
- **Disadvantage** is that the p-value is really, really hard to interpret correctly.
  + Also has an "all or nothing" connotation (unfortunately magic thresholds don't exist)


