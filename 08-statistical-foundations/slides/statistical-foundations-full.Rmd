---
title: "DSC365: Introduction to Data Science"
author: "Statistical Foundations"
date: "October 2, 2025"
output: 
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(RColorBrewer)
library(mosaic)
library(infer)


hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

```

## Announcements

**Lab 4**

- Due Thursday October 9th, 2025 at 11:59 pm in Blueline

**Quiz 2**

- Last 30 minutes of class Thursday October 9th
- Topics: Data Communication and Statistical Foundations

**Lab 5**: work day in class on Thursday October 9th

- Due Tuesday October 21st, 2025 at 11:59 pm in Blueline

**Mini Project 2**

- Due Thursday October 23rd at 11:59 in Blueline




---
### Isn't this data science?

- Statistics: Statistics is a mathematically based field which seeks to collect and interpret quantitative data. 

- Data science: Data science is a multidisciplinary filed which uses scientific methods, processes and system to extract information from data in a large range of forms. 


---
### Samples and Population

In statistics we are interested in a **population** of cases/people/objects. However, often this population is too large to collect data on, so we take **samples** from the larger population.

Statistical methodology assumes that the cases are drawn from a much larger set of potential cases, so the given data are a sample of a larger population of potential cases. 

--

Statistical Inference: the process of using sample data to make conclusions about the underlying population the sample came from
- Before we talk about methods for statistical inference, let's discuss uncertainty

---
### Uncertainty

What do we mean by uncertainty?

- Roughly equivalent to the notion of repeatability. 
  + You’re certain if your results are repeatable. 
  + You’re uncertain if your results are subject to a lot of statistical fluctuations.
  
--

Why are Statisticians Always Uncertain?
 
- Our data consists of a sample
- We want to use our models to make a prediction, but there are always things that we can't account for in a model
- Our observations are subject to measurement or reporting error
- Our data arise from an intrinsically random or variable process

---
### Example: Setting travel policy through sampling

**Example**: You’ve been asked to develop a travel policy for business travelers going from New York City to Chicago. Assume that the `nycflights13` data set represents the complete population of flights.


**First** filter all the flights going to Chicago and with a non-NA value of arrival delay time.

```{r}
library(nycflights13)
Chicago <- flights %>%
  filter(dest %in% c('ORD', 'MDW'), !is.na(arr_delay))

nrow(Chicago)
```

---
### Let's find a Sample (n=100)!

```{r}
set.seed(365)
Sample100 <- Chicago %>% sample_n(size=100)
```

--

How long of a delay should we expect based on the sample?

```{r}
Sample100 %>% summarize(min=min(arr_delay),
                        q05=quantile(arr_delay, 0.05),
                        mean=mean(arr_delay),
                        median=median(arr_delay),
                        max=max(arr_delay), 
                        q95=quantile(arr_delay, 0.95),
                        sd=sd(arr_delay))
```

--

```{r}
mean(Chicago$arr_delay) # population value
```


---
### What constitutes an “unacceptable” delay?

How about an one and forty-five minuted delay (105 minutes)?

```{r}
Chicago %>% mutate(less105 = arr_delay<=105) %>% 
  group_by(less105) %>% 
  count() %>%
  mutate(pct = n / nrow(Chicago))
```


---
### Take Another sample


```{r}
set.seed(10)
Sample100_2 <- Chicago %>% sample_n(size=100)

Sample100_2 %>% summarize(mean=mean(arr_delay),
            min=min(arr_delay),
            max=max(arr_delay), 
            sd=sd(arr_delay))
```


The sample mean is now 9.64. How does this compare to the previous sample? 

--

Different samples of data will have different sample statistics!

-   In most real-world settings, we do not have access to the population data. We have only our one sample.

---
### Sample Statistics

Statistic: A quantity computed from values in a sample to represent some characteristics

Examples:

+ Mean $(\bar{x})$
+ Proportion $(\hat{p})$
+ Median $(m)$
+ Standard Deviation $(s)$

These are also called **point estimates**: single value from a sample to approximate this value in the population.

---
### Sampling Distributions

Sample statistics depend completely on the data. Different samples of data will have different sample statistics!

We need to figure out the reliability of a sample statistic from the sample itself. 
  + For now, though, we are going to use the population to develop some ideas about how to define reliability. 

--

If we draw many different samples from the population, each of size $n$, and calculated the sample statistic on each of those samples, how similar would the sample statistic be across all the samples?
  + Create a sampling distribution

--

**Sampling distribution**: a set of probabilities represents the chance to see certain values of the statistics

---
### Creating a Sampling Distribution

We could run the previous code multiple times to obtain our different samples

--

Or, we could speed it up:

```{r}
n <- 100
num_trials <- 500
chi_25_means <- 1:num_trials %>% 
    map_dfr(~Chicago %>% slice_sample(n = n) %>% 
        summarize(mean_arr_delay = mean(arr_delay))) %>% 
  mutate(n = n)

head(chi_25_means)
```

---
### Creating a Sampling Distribution

```{r}
favstats(~mean_arr_delay, data=chi_25_means) #mosaic library
```

```{r, fig.height=4.5, fig.width=8, fig.align='center'}
ggplot(chi_25_means, aes(x=mean_arr_delay)) + 
  geom_density(fill='turquoise', alpha=0.5) + 
  labs(x='Sampling Distribution of Sample Mean')
```

---
### What does the Sampling Distribution tells us?

- Where is the sampling distribution centered? 
  + The expected value of a statistical summary is the average of that summary’s sampling distribution—that is, the average value of that summary under repeated sampling from the same random process that generated our data.

- How spread out is the sampling distribution? 
  + The answer to this question provides a quantitative measure of repeatability, and therefore statistical uncertainty.
  + The standard deviation of a sampling distribution is called the **standard error**. This reflects the typical statistical fluctuation of our summary statistic

---
## Discuss Reliability: Standard Error

**Standard error**: It describes the width of the sampling distribution.

$$SE = \frac{s}{\sqrt{n}}$$
--

Example: What’s the standard error of the sample mean?

```{r}
favstats(~arr_delay, data=Sample100)
```

```{r}
favstats(~mean_arr_delay, data=chi_25_means)
```


---
### How does n Affect the Sampling Distribution?

```{r means, message=FALSE, warning=FALSE, cache=TRUE}
Means50 <- do(1000)*(Chicago %>% sample_n(size=50) %>% 
                       summarize(mean=mean(arr_delay)))
Means100 <- do(1000)*(Chicago %>% sample_n(size=100) %>% 
                        summarize(mean=mean(arr_delay)))
Means500 <- do(1000)*(Chicago %>% sample_n(size=500) %>% 
                        summarize(mean=mean(arr_delay)))
```

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=8, echo=FALSE}
Means <- rbind(Means50 %>% mutate(n=50),
               Means100 %>% mutate(n=100),
               Means500 %>% mutate(n=500))
```

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=8, echo=FALSE}
ggplot(dat=Means, aes(x=mean)) + 
  geom_density(aes(fill=as.factor(n))) + 
  facet_grid(~n)+xlab('Sample means') + guides(fill=FALSE)
```


---
### How does n Affect the Sampling Distribution?


A larger sample size produces a smaller standard error. 
  - A larger sample size is more reliable than a smaller sample size

--

What happens to the confidence interval as the standard error decreases?

--

+ Smaller Interval

--

For large sample sizes, the shape of the sampling distribution tends to bell-shaped


---
### Bootstraping

In the last example, we had access to the population data and so we could find the sampling distribution by repeatedly sampling from the population. 

--

**Bootstrap** is a method that allows us to approximate the sampling distribution even though we do not have the population.

In bootstraping we think of our sample as if it were the population. 
  - Still drawing many new samples from our original sample (**Resampling**). 

When resampling, we sample with replacement.
  - Allows us to estimate the variability of the sample
  - Otherwise, would always get the same sample statistics as the observed value.

Bootstrapping does not create new cases (not for data collection)

---
## Bootstraping

```{r, echo=FALSE, fig.align='center'}

knitr::include_graphics("~/Documents/Classes/MTH365/mth-365/08-statistical-foundations/slides/images/bootstrap.png")

```

  
---
## Bootstraping: Let's take a small sample (n=3)

```{r}
f3 <- Chicago %>% sample_n(size=3) %>% dplyr::select(year,month,day)
```

```{r, echo=FALSE}
f3 
```

--

.pull-left[
First Resample: 
```{r}
f3 %>% slice_sample(n= 3, 
               replace = TRUE)
```
].pull-right[
Second Resample: 
```{r}
f3 %>% slice_sample(n= 3, 
               replace = TRUE)
```
]

---
## Bootstrapping: Bigger Sample

```{r, fig.height=4.5, fig.width=8, fig.align='center'}
Bootstrap_Means <- Sample100 %>% 
  specify(response = arr_delay) %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "mean")

ggplot(Bootstrap_Means, aes(x=stat))+
  geom_density(fill='turquoise', alpha=0.5)+labs(x='Bootstrap Means')

```

---
### Bootstrapping: Bigger Sample


Bootstrap Sample:
```{r}
favstats(~stat, data=Bootstrap_Means)
```

Sampling Distribution:
```{r}
favstats(~mean_arr_delay, data=chi_25_means)
```

--

For moderate to large sample sizes and sufficient number of bootstraps, the bootstrap distribution approximates certain aspects of the sampling distribution, like the standard error and quantiles 


```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=8, eval = FALSE}
Boot1 <- Chicago %>%
  sample_n(size=100) %>% 
  specify(response = arr_delay) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")

Boot2 <- Chicago %>%
  sample_n(size=100) %>% 
  specify(response = arr_delay) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")

Boot3 <- Chicago %>%
  sample_n(size=100) %>% 
  specify(response = arr_delay) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")

BootSamples <- rbind(Boot1 %>% mutate(Sample=1), 
               Boot2 %>% mutate(Sample=2), 
               Boot3 %>% mutate(Sample=3))
ggplot(data=BootSamples, aes(x=stat)) +
  geom_density(aes(fill=as.factor(Sample))) +
  facet_grid(~Sample)+xlab('Bootstrap Distributions') +
  guides(fill=FALSE)
```

---
### Side Note: We want Quality Samples!

The quality of bootstrap estimates depends on the quality of the collected data.
  - So we want quality samples!
  
Quality samples tend to be representative of the population of interest


---
### Outliers

Outliers: a data point that differs significantly from other observations

When you have an outlier:

- Identify the potential outlier
- Try to understand why it is an outlier
- Only remove it if you are sure it is caused by completely random error, or not related to your research focus.

---
### Confounding Variables and Simpson's Paradox

.center[
Correlation does not imply causation
]

<br>

- A common concern with observational data is that correlations are distorted by other variables -> *confounding variables*
  + Randomized trials control for this; but not always practical
  
- In observational studies, addressing confounding is straightforward if these variables are measured (ex. can add them to model)


---
### Confounding Variables and Simpson's Paradox


Simpson's paradox is a phenomenon where a trend appears in several groups of data but disappears or reverses when the groups are combined. 
- The paradox can be resolved when confounding variables are appropriately addressed, but can lead to problematic interpretations if they are not. 

```{r, echo=FALSE, fig.align='center', out.height="90%", out.width="85%"}

knitr::include_graphics("~/Documents/Classes/MTH365/mth-365/08-statistical-foundations/slides/images/simpson-paradox.png")

```


---
### Hypothesis Tests and its Perils

Made up of 4 steps:

1). Write out your hypotheses for testing

- Start with a null hypothesis $(H_0)$, that represents the status quo ("there is nothign going on")
- Set an alternative hypothesis $(H_A)$, that represents the research question, i.e. what we’re testing for
  - Can be one-sided $(<,>)$ or two-sided $(\neq)$

2/3). Conduct a hypothesis test under the assumption that the null hypothesis is true
  - calculate a test-statistic/p-value 

4). If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis
  - If they do, then reject the null hypothesis in favor of the alternative


---
### Example: Lee Corso Head Gear Picks

Lee Corso is a football analyst and broadcaster commonly known for his “headgear” picks on ESPNs College GameDay until his retirement earlier this season. But the real question is, does he know what he is talking about? We randomly selected 100 games and found he correctly picked 62 teams correctly out of 100 games (62%)

```{r, echo=FALSE, fig.align='center', out.width="40%"}

knitr::include_graphics("~/Documents/Classes/MTH365/mth-365/08-statistical-foundations/slides/images/corso.png")

```

--

$H_0:$  Corso's picks for the winning football team was truly random during his career.

$H_A:$ Corso's picks for the winning football team was higher than what would be expected by random chance throughout his career.


---
### Null Distribution

Our burning question: just how likely is it that Lee Corso could pick the winning team least 66 out of 104 times, assuming that he's just guessing?
If he was just guessing, we'd expect him to correctly pick the correct team about half of the time, but we also have to account for the possibility that they just got lucky. Hence we want to simulate a random process, assuming the null hypothesis is true, called a **Null Distribution**


```{r, echo = FALSE, fig.align='center', fig.width=10, fig.height=5}
set.seed(4)
patriots_sim = do(10000)*nflip(100)

ggplot(patriots_sim) + 
  geom_histogram(aes(x=nflip), binwidth=1)
```


---
### New England Patriots: P-value

A **p-value** is the probability of observing a test statistic as extreme, or more extreme, than what observed, given that the null hypothesis is true.

<br>

```{r, echo = FALSE, fig.align='center', fig.width=10, fig.height=5}
#sum(patriots_sim >= 19)

patriots_sim <- patriots_sim %>% mutate(pvalue = ifelse(nflip < 62, "less", "greater"))

ggplot(patriots_sim) + 
  geom_histogram(aes(x=nflip, fill = pvalue), binwidth=1) +
  geom_vline(xintercept = 61.5) + 
  geom_text(x=64, y=500, label='P(>= 62 Wins) = 0.0111', colour='black', size=5) + theme(legend.position="none")


```


---
### Making Conclusions

We often use 5% as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model. This cutoff value is called the significance level, $\alpha$.

- If p-value < $\alpha$ , reject $H_0$ in favor of $H_A$: The data provide convincing evidence for the alternative hypothesis.

- If p-value > $\alpha$, fail to reject $H_0$ in favor of $H_A$: The data do not provide convincing evidence for the alternative hypothesis.

--

In our example, since 0.0111 < 0.05, we have evidence that Corso's picks for the winning football team was higher than what would be expected by random chance throughout his career.


---
### P-Values

Using a p-value as a measure of statistical significance has both advantages and disadvantages:
- **Advantage** is that the p-value gives us a continuous measure of evidence against the null hypothesis. 
- **Disadvantage** is that the p-value is really, really hard to interpret correctly.
  + Also has an "all or nothing" connotation (unfortunately magic thresholds don't exist)


