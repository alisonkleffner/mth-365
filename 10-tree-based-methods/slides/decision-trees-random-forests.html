<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>DSC365: Introduction to Data Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Decision Trees and Random Forests" />
    <meta name="date" content="2025-10-23" />
    <script src="libs/header-attrs-2.27/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# DSC365: Introduction to Data Science
]
.author[
### Decision Trees and Random Forests
]
.date[
### October 23, 2025
]

---




&lt;style type="text/css"&gt;
.tiny .remark-code { /*Change made here*/
  font-size: 70% !important;
}
&lt;/style&gt;

## Announcements

**Lab 5** (Linear Models)

- Due Tuesday October 28th, 2025 at 11:59 pm in Blueline
- Will start Lab 6 in class on Tuesday

**Mini Project 2**

- Due Tonight at 11:59 in Blueline

**Quiz 3**: in class Tuesday October 28th

- Covers: Prediction/KNN/Decsion Trees

---
### Supervised vs Unsupervised Learning

Learning techniques fall into two categories:

1). Supervised learning:

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

2). Unsupervised learning: 

---
### Tree-Based Methods

- Can be used for both regression and classification
  + Regression models `\(\longrightarrow\)`  quantitative response variable
&lt;br&gt;

  + Classification models `\(\longrightarrow\)` categorical response 
&lt;br&gt;


- These involve stratifying or segmenting the predictor space into a number of simple regions


---
### Example: Marijuana legalization

The General Social Survey is a wide-ranging survey conducted biannually to measure cultural shifts in American society. We can use the GSS to get an idea of how popular opinion has changed.

.tiny[

``` r
GSS &lt;- read.csv("GSS2016.csv")
glimpse(GSS)
```


```
## Rows: 9,423
## Columns: 18
## $ YEAR     &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010…
## $ NEWSFROM &lt;chr&gt; "Not applicable", "Not applicable", "Not applicable", "Not applicable", "Tv", "Not…
## $ HAPPY    &lt;chr&gt; "Pretty happy", "Not too happy", "Not too happy", "Not too happy", "Very happy", "…
## $ RELIG    &lt;chr&gt; "Catholic", "None", "Catholic", "Catholic", "Protestant", "None", "Catholic", "Pro…
## $ GRASS    &lt;chr&gt; "Don't know", "Legal", "Not applicable", "Not legal", "Not legal", "Legal", "Don't…
## $ COURTS   &lt;chr&gt; "About right", "Too harsh", "Not harsh enough", "Not harsh enough", "Not harsh eno…
## $ ENERGY   &lt;chr&gt; "Too little", "Too little", "Don't know", "About right", "Don't know", "Too little…
## $ EDUC     &lt;chr&gt; "Not applicable", "Too little", "Too little", "Not applicable", "About right", "To…
## $ ENVIR    &lt;chr&gt; "Not applicable", "Too little", "Too little", "Not applicable", "About right", "To…
## $ POLVIEWS &lt;chr&gt; "Slightly liberal", "Liberal", "Don't know", "Liberal", "Slightly liberal", "Sligh…
## $ PARTYID  &lt;chr&gt; "Democrat", "Democrat", "Democrat", "Republican", "Independent", "Democrat", "Inde…
## $ REGION   &lt;chr&gt; "Middle atlantic", "Middle atlantic", "Middle atlantic", "Middle atlantic", "Middl…
## $ INCOME   &lt;chr&gt; "$25000 or more", "$15000 - 19999", "$20000 - 24999", "$8000 to 9999", "Don't know…
## $ SEX      &lt;chr&gt; "Male", "Female", "Female", "Female", "Female", "Male", "Female", "Female", "Femal…
## $ DEGREE   &lt;chr&gt; "Bachelor", "Bachelor", "Lt high school", "Lt high school", "Lt high school", "Lt …
## $ AGE      &lt;chr&gt; "31", "23", "71", "82", "78", "40", "46", "80", "31", "No answer", "31", "21", "58…
## $ MARITAL  &lt;chr&gt; "Never married", "Never married", "Divorced", "Widowed", "Married", "Never married…
## $ BALLOT   &lt;chr&gt; "Ballot b", "Ballot b", "Ballot a", "Ballot b", "Ballot c", "Ballot b", "Ballot c"…
```
]

---
### Let's Clean Our Data! Yay!

- Let's only look at one year, say 2016, and remove "Not applicable from our response"


``` r
GSS &lt;- GSS %&gt;% filter(YEAR==2016) %&gt;% 
  filter(GRASS != 'Not applicable')
```

- Want just two groups for responses: Legal and Not legal


``` r
GSS &lt;- GSS %&gt;%
mutate(LEGAL = ifelse(GRASS=='Legal', 'Legal', 'Not legal'))
```

- Change variables to proper type


``` r
GSS$AGE &lt;- as.numeric(GSS$AGE)
```

```
## Warning: NAs introduced by coercion
```

---
### Testing data v. Training data

**Goal**: Use Age to predict people’s opinion of marijuana legalization.


``` r
set.seed(4)
test_id &lt;- sample(1:nrow(GSS), size=round(0.2*nrow(GSS)))
TEST &lt;- GSS[test_id,]
TRAIN &lt;- GSS[-test_id,]
```

&lt;br&gt;

How many people in the training data set support marijuana legalization?


``` r
TRAIN %&gt;% group_by(LEGAL) %&gt;% summarize(n=n())
```

```
## # A tibble: 2 × 2
##   LEGAL         n
##   &lt;chr&gt;     &lt;int&gt;
## 1 Legal       911
## 2 Not legal   654
```


---
### Decision Trees

Decision trees: A tree-like model of decisions and their possible consequences

- Has flowchart-like structure in which each...

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;img src="./images/decision-tree.png" width="75%" style="display: block; margin: auto auto auto 0;" /&gt;

---
### Decision Trees (Classification)


&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

How to decide to split?

- As the number of predictors increases, so does the number of possible trees.
  + Optimal decision trees do not exist (Hyafil &amp; Rivest, 1976)
  + So several competing methods for building decision trees

---
### Decision Trees (Classification)

Recursive Partitioning: Split infinite # of times until reach stopping criterion.

Split:
  + Gini Index (default): 
  
`$$G = \sum_{k=1}^{K} \hat{p}_{mk}(1-\hat{p}_{mk})$$` where `\(\hat{p}_{mk}\)` is the proportion of training observations in `\(m^{th}\)` region from `\(k^{th}\)` class.
    
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;


Stop:

- Complexity parameter

  
---
### Fitting A Decision Tree (Classification)


``` r
#install.packages('rpart')
library(rpart)
rpart(LEGAL~AGE, data=TRAIN, na.action = na.pass)
```

```
## n= 1565 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 1565 654 Legal (0.5821086 0.4178914)  
##   2) AGE&lt; 68.5 1350 521 Legal (0.6140741 0.3859259) *
##   3) AGE&gt;=68.5 215  82 Not legal (0.3813953 0.6186047) *
```

---
### Visualizing a Decision Tree (Classification)


``` r
#install.packages("rattle")
library(rattle)
tree &lt;- rpart(LEGAL~AGE, data=TRAIN, na.action = na.pass)
fancyRpartPlot(tree)
```




&lt;img src="./images/tree1.png" width="75%" style="display: block; margin: auto;" /&gt;

---
### Visualizing using ggplot


``` r
TRAIN %&gt;% ggplot(aes(x=LEGAL, y=AGE)) +
geom_boxplot(aes(col=LEGAL)) +
  geom_hline(yintercept=69, col='black') 
```

&lt;img src="decision-trees-random-forests_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

---
### Evaluating a decision tree

- Confusion Matrix
- Classification Accuracy

---
### Confusion Matrix


``` r
TRAIN &lt;- TRAIN %&gt;%
  mutate(Legal_Tree = predict(tree, type='class'))

confusion_train &lt;- tally(Legal_Tree~LEGAL, data=TRAIN)
confusion_train
```

```
##            LEGAL
## Legal_Tree  Legal Not legal
##   Legal       829       521
##   Not legal    82       133
```


``` r
TEST &lt;- TEST %&gt;%
  mutate(Legal_Tree = predict(tree, type='class', newdata = TEST))

confusion_test &lt;- tally(Legal_Tree~LEGAL, data=TEST)
confusion_test
```

```
##            LEGAL
## Legal_Tree  Legal Not legal
##   Legal       203       145
##   Not legal    12        31
```

---
### Classification Accuracy

Training Accuracy:


``` r
sum(diag(confusion_train))/nrow(TRAIN)
```

```
## [1] 0.6146965
```

Testing Accuracy:


``` r
sum(diag(confusion_test))/nrow(TEST)
```

```
## [1] 0.5984655
```


---
### Tuning Parameter: Complexity parameter

Using recursive paritioning can overfit the training data - Prune Tree 

.tiny[

``` r
printcp(tree)
```

```
## 
## Classification tree:
## rpart(formula = LEGAL ~ AGE, data = TRAIN, na.action = na.pass)
## 
## Variables actually used in tree construction:
## [1] AGE
## 
## Root node error: 654/1565 = 0.41789
## 
## n= 1565 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.077982      0   1.00000 1.00000 0.029834
## 2 0.010000      1   0.92202 0.94037 0.029544
```
]

- Change CP (Example)


``` r
rpart(LEGAL~AGE, data=TRAIN, na.action = na.pass,
      control = rpart.control(cp = 0.05))
```


---
### Decision Trees (Regression)


&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;


How to decide to split?

- Find regions `\((R_j)\)` that minimizes the residual sum of squares `$$RSS = \sum^{J}_{j=1}\sum_{i \in R_j}(y_i - \hat{y}_{R_j})^2$$`
  - Stops when you reach some criterion (cp)


---
### Fitting A Decision Tree (Regression)

Let's suppose we want to use people’s political view (POLVIEWS) and marital status (MARITAL) to estimate people’s age.


``` r
tree2 &lt;- rpart(AGE~POLVIEWS+MARITAL, data=TRAIN)
fancyRpartPlot(tree2)
```



&lt;img src="./images/tree2.png" width="75%" style="display: block; margin: auto;" /&gt;

---
### Prediction for Decision Regression Tree

We can still use the predict function to predict our regression decision tree outputs. 

``` r
TEST &lt;- TEST %&gt;% filter(MARITAL != "No answer") 

predict(tree2, TEST , method = "anova") %&gt;% head()
```

```
##        1        2        3        4        5        6 
## 51.93004 51.93004 51.93004 51.93004 35.25934 51.93004
```

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;


**Be Careful**: Can only predict using categorical variables located in the Training Set

---
### Trees Versus Linear Models

- If the relationship between the features and the response is well approximated by
a linear model: 
  + Linear regression 
  
&lt;br&gt;
&lt;br&gt;

- If instead there is a highly nonlinear and complex relationship between the features and the response as indicated by model: 
  + Decision trees



---
### Advantages and Disadvantages of Decision Trees

**Advantages**

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;


**Disadvantages** 
  
---
### Random Forests

A random forest is collection of decision trees that are aggregated by majority rule

  
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

When to use random forest:



---
### Building a Random Forest

In building a random forest, each time a split in a tree is considered, a random sample of *m* predictors is chosen as split candidates from the full set of *p* predictors. 

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;


Hence, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. Why is this a good thing?

---
### Random Forests 

**Example**: Which variables are most important for predicting views on marijuana legalization?


.tiny[

``` r
#install.packages('randomForest')
library(randomForest)

forest_grass &lt;- randomForest(as.factor(LEGAL)~NEWSFROM+HAPPY+
                               RELIG+COURTS+ENERGY+EDUC+ENVIR+
                               POLVIEWS+PARTYID+REGION+INCOME+
                               SEX+DEGREE+AGE+MARITAL+BALLOT, 
                             data=TRAIN, na.action = na.omit,
                             ntree=201, mtry=4)

forest_grass
```

```
## 
## Call:
##  randomForest(formula = as.factor(LEGAL) ~ NEWSFROM + HAPPY +      RELIG + COURTS + ENERGY + EDUC + ENVIR + POLVIEWS + PARTYID +      REGION + INCOME + SEX + DEGREE + AGE + MARITAL + BALLOT,      data = TRAIN, ntree = 201, mtry = 4, na.action = na.omit) 
##                Type of random forest: classification
##                      Number of trees: 201
## No. of variables tried at each split: 4
## 
##         OOB estimate of  error rate: 33.96%
## Confusion matrix:
##           Legal Not legal class.error
## Legal       704       202   0.2229581
## Not legal   325       321   0.5030960
```
]

---
### Random Forests: Prediction


``` r
TEST &lt;- TEST %&gt;%
  mutate(Legal_RF = predict(forest_grass, type='class', 
                            newdata = TEST)) 

TEST$Legal_RF[1:5]
```

```
##         1         2         3         4         5 
## Not legal     Legal     Legal     Legal     Legal 
## Levels: Legal Not legal
```

``` r
confusion_test &lt;- tally(Legal_RF~LEGAL, data=TEST)
sum(diag(confusion_test))/nrow(TEST)
```

```
## [1] 0.5717949
```

---
### Variable Importance

Since each tree in a random forest uses a different set of variables, we want to keep track of which variables seem to be the most consistently influential. 

.tiny[

``` r
randomForest::importance(forest_grass) %&gt;% as.data.frame() %&gt;% 
  rownames_to_column() %&gt;% arrange(desc(MeanDecreaseGini))
```

```
##     rowname MeanDecreaseGini
## 1       AGE        128.51054
## 2    REGION         68.41636
## 3  POLVIEWS         65.99878
## 4    INCOME         52.25881
## 5    DEGREE         48.04504
## 6    COURTS         45.31095
## 7   MARITAL         44.85723
## 8     RELIG         44.83231
## 9   PARTYID         41.61337
## 10   ENERGY         36.68400
## 11    HAPPY         35.53879
## 12    ENVIR         34.43033
## 13     EDUC         29.67761
## 14 NEWSFROM         24.74390
## 15      SEX         22.24408
## 16   BALLOT         20.41207
```
]


---
### Decision Tree with Selected Importance


``` r
tree4 &lt;- rpart(LEGAL~AGE+REGION+POLVIEWS, data=TRAIN)
fancyRpartPlot(tree4)
```



&lt;img src="./images/tree4.png" width="2437" style="display: block; margin: auto;" /&gt;

---
### Summary 

Linear model: The basic model for regression, easy for interpretation, but has strict assumption thus hard to get a better prediction

Decision tree: Can be applied to regression and classification. Has good data visualization but it has high variance. 

Random Forest: a collection of tree models. Hard for interpretation but it can output variable importance. Can be useful if you have a lot of variable and what want to select the most useful ones. Also, if you have variables are not independent with each other, it performs better than the linear model. 

KNN：Can be applied to regression and classification. relatively fast, no assumption need and add data anytime. May affect the accuracy if we have too many variables and need to use CV to decide the k value. 

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
